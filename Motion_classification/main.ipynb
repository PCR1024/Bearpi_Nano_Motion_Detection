{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a124e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 23:37:32.458662: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 23:37:32.458742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 23:37:32.460113: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 23:37:32.589870: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from nnom import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d382aa",
   "metadata": {},
   "source": [
    "### 一、加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8effdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_name = [\"Circle\", \"Letter_L\", \"Letter_M\", \"Letter_R\", \"Letter_W\", \"NoMotion\", \"Triangle\", \"Upanddown\"]\n",
    "data_per_size = 50\n",
    "class_num = len(motion_name)\n",
    "# 全部数据\n",
    "total_data = []\n",
    "# 标签数组\n",
    "total_y = np.array([j for j in range(class_num) for i in range(data_per_size)])\n",
    "\n",
    "# 获取标签的唯一值并排序\n",
    "total_labels = np.unique(total_y)\n",
    "\n",
    "# 创建独热编码矩阵\n",
    "total_labels = np.eye(len(total_labels))[total_y]\n",
    "\n",
    "for mo_name in motion_name:\n",
    "    for i in range(data_per_size):\n",
    "        data = np.loadtxt(f\"/home/whiz/Projects/Motion_classification/Dataset/{mo_name}/{mo_name}_{i}.txt\", delimiter=',')\n",
    "        total_data.append(data[ : , -3 :])\n",
    "        \n",
    "total_data = np.array(total_data)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db202b43",
   "metadata": {},
   "source": [
    "### 二、分割数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10726c3",
   "metadata": {},
   "source": [
    "### 三、创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab3ca94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 23:38:02.023541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.266181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.266248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.269059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.269116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.269152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.501630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.501738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.501750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-06 23:38:02.501814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-06 23:38:02.501835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 23:38:04.371880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-11-06 23:38:06.052016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa22755da60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-06 23:38:06.052061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2024-11-06 23:38:06.058515: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-06 23:38:06.132237: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 - 6s - loss: 1.8205 - accuracy: 0.4023 - val_loss: 1.6757 - val_accuracy: 0.4375 - 6s/epoch - 1s/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.8011 - accuracy: 0.8086 - val_loss: 1.7373 - val_accuracy: 0.4531 - 115ms/epoch - 19ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.5172 - accuracy: 0.8789 - val_loss: 1.1819 - val_accuracy: 0.6406 - 112ms/epoch - 19ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3205 - accuracy: 0.9180 - val_loss: 0.6685 - val_accuracy: 0.7969 - 116ms/epoch - 19ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.1834 - accuracy: 0.9688 - val_loss: 0.6957 - val_accuracy: 0.7656 - 112ms/epoch - 19ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1582 - accuracy: 0.9570 - val_loss: 0.7549 - val_accuracy: 0.7031 - 124ms/epoch - 21ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1236 - accuracy: 0.9609 - val_loss: 0.6397 - val_accuracy: 0.7344 - 115ms/epoch - 19ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.1106 - accuracy: 0.9766 - val_loss: 0.4451 - val_accuracy: 0.8438 - 130ms/epoch - 22ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.1149 - accuracy: 0.9609 - val_loss: 0.1892 - val_accuracy: 0.9375 - 104ms/epoch - 17ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0731 - accuracy: 0.9805 - val_loss: 0.1156 - val_accuracy: 0.9688 - 116ms/epoch - 19ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0377 - accuracy: 0.9922 - val_loss: 0.1246 - val_accuracy: 0.9688 - 104ms/epoch - 17ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0344 - accuracy: 0.9922 - val_loss: 0.1090 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0529 - accuracy: 0.9883 - val_loss: 0.1111 - val_accuracy: 0.9531 - 120ms/epoch - 20ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0295 - accuracy: 0.9961 - val_loss: 0.0649 - val_accuracy: 0.9844 - 106ms/epoch - 18ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0329 - accuracy: 0.9922 - val_loss: 0.0545 - val_accuracy: 0.9844 - 108ms/epoch - 18ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0225 - accuracy: 0.9961 - val_loss: 0.0488 - val_accuracy: 1.0000 - 106ms/epoch - 18ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0196 - accuracy: 0.9961 - val_loss: 0.0495 - val_accuracy: 0.9844 - 112ms/epoch - 19ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0218 - accuracy: 0.9961 - val_loss: 0.0565 - val_accuracy: 0.9844 - 108ms/epoch - 18ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0259 - accuracy: 0.9961 - val_loss: 0.0405 - val_accuracy: 0.9844 - 111ms/epoch - 19ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000 - 114ms/epoch - 19ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9844 - 131ms/epoch - 22ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0139 - accuracy: 0.9961 - val_loss: 0.0316 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0616 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9844 - 123ms/epoch - 21ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9844 - 112ms/epoch - 19ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9844 - 110ms/epoch - 18ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9844 - 110ms/epoch - 18ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 0.9844 - 114ms/epoch - 19ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9844 - 119ms/epoch - 20ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9844 - 119ms/epoch - 20ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0087 - accuracy: 0.9961 - val_loss: 0.0607 - val_accuracy: 0.9844 - 138ms/epoch - 23ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0575 - val_accuracy: 0.9844 - 134ms/epoch - 22ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0107 - accuracy: 0.9961 - val_loss: 0.0797 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0626 - accuracy: 0.9844 - val_loss: 0.0989 - val_accuracy: 0.9688 - 141ms/epoch - 24ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0388 - accuracy: 0.9922 - val_loss: 0.0977 - val_accuracy: 0.9688 - 112ms/epoch - 19ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.0853 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.0633 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 0.9844 - 133ms/epoch - 22ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0355 - accuracy: 0.9922 - val_loss: 0.0496 - val_accuracy: 0.9844 - 110ms/epoch - 18ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0224 - accuracy: 0.9922 - val_loss: 0.0937 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.0796 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0096 - accuracy: 0.9961 - val_loss: 0.0593 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9844 - 115ms/epoch - 19ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0098 - accuracy: 0.9961 - val_loss: 0.0502 - val_accuracy: 0.9844 - 119ms/epoch - 20ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9844 - 114ms/epoch - 19ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9844 - 117ms/epoch - 20ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0089 - accuracy: 0.9961 - val_loss: 0.0433 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.0280 - val_accuracy: 0.9844 - 122ms/epoch - 20ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0363 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9844 - 176ms/epoch - 29ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9844 - 123ms/epoch - 21ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9844 - 117ms/epoch - 20ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9844 - 122ms/epoch - 20ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9844 - 116ms/epoch - 19ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9844 - 131ms/epoch - 22ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9844 - 148ms/epoch - 25ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9844 - 124ms/epoch - 21ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9844 - 136ms/epoch - 23ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9844 - 133ms/epoch - 22ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9844 - 138ms/epoch - 23ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9688 - 137ms/epoch - 23ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 0.9844 - 140ms/epoch - 23ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 8.9574e-04 - accuracy: 1.0000 - val_loss: 0.0841 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0799 - val_accuracy: 0.9844 - 142ms/epoch - 24ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9844 - 118ms/epoch - 20ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 7.8333e-04 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0044 - accuracy: 0.9961 - val_loss: 0.0713 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 6.9569e-04 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9844 - 144ms/epoch - 24ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 5.8464e-04 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 0.9844 - 127ms/epoch - 21ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9844 - 131ms/epoch - 22ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0606 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 9.4966e-04 - accuracy: 1.0000 - val_loss: 0.0612 - val_accuracy: 0.9844 - 121ms/epoch - 20ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 8.4541e-04 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 4.4182e-04 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 6.7350e-04 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9844 - 127ms/epoch - 21ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 3.1094e-04 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9844 - 168ms/epoch - 28ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9844 - 148ms/epoch - 25ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 8.2695e-04 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 7.6809e-04 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.9844 - 123ms/epoch - 21ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 8.1137e-04 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9844 - 112ms/epoch - 19ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9844 - 120ms/epoch - 20ms/step\n",
      "3/3 [==============================] - 1s 116ms/step - loss: 0.0854 - accuracy: 0.9625\n",
      "input_1 Quantized method: max-min  Values max: 32767.0 min: -23321.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d Quantized method: max-min  Values max: 16166.241 min: -19697.77 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "batch_normalization Quantized method: max-min  Values max: 7.919382 min: -7.238631 dec bit 4\n",
      "dropout Quantized method: max-min  Values max: 7.919382 min: -7.238631 dec bit 4\n",
      "re_lu Quantized method: max-min  Values max: 7.919382 min: -7.238631 dec bit 4\n",
      "max_pooling1d Quantized method: max-min  Values max: 7.919382 min: -7.238631 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_1 Quantized method: max-min  Values max: 6.7949657 min: -6.0156603 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_2 Quantized method: max-min  Values max: 5.687415 min: -5.934539 dec bit 4\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa3e18f98b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_1 Quantized method: max-min  Values max: 5.6368413 min: -5.0040298 dec bit 4\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa3e18f99d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_2 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "dropout_1 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "dropout_2 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "re_lu_1 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "re_lu_2 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "max_pooling1d_3 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "max_pooling1d_1 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "max_pooling1d_2 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "dropout_3 Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "concatenate Quantized method: max-min  Values max: 6.503376 min: -6.551334 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_3 Quantized method: max-min  Values max: 8.573399 min: -7.5698147 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_3 Quantized method: max-min  Values max: 5.693515 min: -4.266011 dec bit 4\n",
      "re_lu_3 Quantized method: max-min  Values max: 5.693515 min: -4.266011 dec bit 4\n",
      "dropout_4 Quantized method: max-min  Values max: 5.693515 min: -4.266011 dec bit 4\n",
      "flatten Quantized method: max-min  Values max: 5.693515 min: -4.266011 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense Quantized method: max-min  Values max: 9.783588 min: -6.0840797 dec bit 3\n",
      "dropout_5 Quantized method: max-min  Values max: 9.783588 min: -6.0840797 dec bit 3\n",
      "re_lu_4 Quantized method: max-min  Values max: 9.783588 min: -6.0840797 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_1 Quantized method: max-min  Values max: 16.688314 min: -8.704677 dec bit 2\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "softmax Quantized method: max-min  Values max: 0.9999995 min: 1.93197e-11 dec bit 7\n",
      "set dec bit 4 for the input of concatenate : ['max_pooling1d_1', 'max_pooling1d_2', 'dropout_3']\n",
      "quantisation list {'input_1': [-8, 0], 'conv1d': [4, 0], 'batch_normalization': [4, 0], 'dropout': [4, 0], 're_lu': [4, 0], 'max_pooling1d': [4, 0], 'conv1d_1': [4, 0], 'conv1d_2': [4, 0], 'batch_normalization_1': [4, 0], 'batch_normalization_2': [4, 0], 'dropout_1': [4, 0], 'dropout_2': [4, 0], 're_lu_1': [4, 0], 're_lu_2': [4, 0], 'max_pooling1d_3': [4, 0], 'max_pooling1d_1': [4, 0], 'max_pooling1d_2': [4, 0], 'dropout_3': [4, 0], 'concatenate': [4, 0], 'conv1d_3': [4, 0], 'batch_normalization_3': [4, 0], 're_lu_3': [4, 0], 'dropout_4': [4, 0], 'flatten': [4, 0], 'dense': [3, 0], 'dropout_5': [3, 0], 're_lu_4': [3, 0], 'dense_1': [2, 0], 'softmax': [7, 0]}\n",
      "fusing batch normalization to conv1d\n",
      "original weight max 0.17299165 min -0.1567609\n",
      "original bias max 2.6247915e-07 min -1.4192534e-07\n",
      "fused weight max 0.00016968693 min -0.00016272336\n",
      "fused bias max 0.06315131 min -0.08221894\n",
      "quantizing weights for layer conv1d\n",
      "    tensor_conv1d_kernel_0 dec bit 19\n",
      "    tensor_conv1d_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization\n",
      "fusing batch normalization to conv1d_1\n",
      "original weight max 0.17751306 min -0.16548328\n",
      "original bias max 0.00014988035 min -0.00022708447\n",
      "fused weight max 0.24447466 min -0.260625\n",
      "fused bias max 1.1085205 min -1.1074654\n",
      "quantizing weights for layer conv1d_1\n",
      "    tensor_conv1d_1_kernel_0 dec bit 8\n",
      "    tensor_conv1d_1_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_2\n",
      "original weight max 0.21955991 min -0.19968718\n",
      "original bias max 0.00013966873 min -0.0003126825\n",
      "fused weight max 0.4348371 min -0.4337027\n",
      "fused bias max 0.8614599 min -0.8204495\n",
      "quantizing weights for layer conv1d_2\n",
      "    tensor_conv1d_2_kernel_0 dec bit 8\n",
      "    tensor_conv1d_2_bias_0 dec bit 7\n",
      "quantizing weights for layer batch_normalization_1\n",
      "quantizing weights for layer batch_normalization_2\n",
      "fusing batch normalization to conv1d_3\n",
      "original weight max 0.158553 min -0.19743867\n",
      "original bias max 0.00019909283 min -9.4962255e-05\n",
      "fused weight max 0.14054571 min -0.13404027\n",
      "fused bias max 1.0437573 min -1.0782137\n",
      "quantizing weights for layer conv1d_3\n",
      "    tensor_conv1d_3_kernel_0 dec bit 9\n",
      "    tensor_conv1d_3_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_3\n",
      "quantizing weights for layer dense\n",
      "    tensor_dense_kernel_0 dec bit 9\n",
      "    tensor_dense_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_1\n",
      "    tensor_dense_1_kernel_0 dec bit 8\n",
      "    tensor_dense_1_bias_0 dec bit 11\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 2.0036 - accuracy: 0.3320 - val_loss: 2.6952 - val_accuracy: 0.4688 - 3s/epoch - 444ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.9394 - accuracy: 0.7461 - val_loss: 1.4542 - val_accuracy: 0.5312 - 152ms/epoch - 25ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.4976 - accuracy: 0.8633 - val_loss: 0.7080 - val_accuracy: 0.7656 - 146ms/epoch - 24ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3611 - accuracy: 0.9023 - val_loss: 0.5124 - val_accuracy: 0.8750 - 159ms/epoch - 27ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2364 - accuracy: 0.9531 - val_loss: 0.4083 - val_accuracy: 0.9219 - 159ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1727 - accuracy: 0.9531 - val_loss: 0.3117 - val_accuracy: 0.9062 - 161ms/epoch - 27ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1406 - accuracy: 0.9727 - val_loss: 0.2388 - val_accuracy: 0.9375 - 186ms/epoch - 31ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.1268 - accuracy: 0.9688 - val_loss: 0.1881 - val_accuracy: 0.9375 - 167ms/epoch - 28ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0677 - accuracy: 0.9883 - val_loss: 0.1380 - val_accuracy: 0.9531 - 142ms/epoch - 24ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0633 - accuracy: 0.9805 - val_loss: 0.0967 - val_accuracy: 0.9844 - 142ms/epoch - 24ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0595 - accuracy: 0.9883 - val_loss: 0.0820 - val_accuracy: 0.9844 - 123ms/epoch - 21ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0443 - accuracy: 0.9883 - val_loss: 0.0717 - val_accuracy: 0.9844 - 120ms/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0499 - accuracy: 0.9805 - val_loss: 0.0707 - val_accuracy: 0.9844 - 193ms/epoch - 32ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0379 - accuracy: 0.9844 - val_loss: 0.0732 - val_accuracy: 0.9844 - 223ms/epoch - 37ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0447 - accuracy: 0.9883 - val_loss: 0.0742 - val_accuracy: 0.9844 - 136ms/epoch - 23ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0338 - accuracy: 0.9922 - val_loss: 0.0681 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0416 - accuracy: 0.9883 - val_loss: 0.0421 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0436 - accuracy: 0.9883 - val_loss: 0.0772 - val_accuracy: 0.9688 - 111ms/epoch - 19ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0549 - accuracy: 0.9844 - val_loss: 0.1086 - val_accuracy: 0.9531 - 112ms/epoch - 19ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 0.9844 - 122ms/epoch - 20ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0360 - accuracy: 0.9961 - val_loss: 0.0407 - val_accuracy: 0.9844 - 117ms/epoch - 20ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0298 - accuracy: 0.9961 - val_loss: 0.0519 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0193 - accuracy: 0.9961 - val_loss: 0.0614 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0173 - accuracy: 0.9961 - val_loss: 0.0633 - val_accuracy: 0.9844 - 115ms/epoch - 19ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0334 - accuracy: 0.9805 - val_loss: 0.0519 - val_accuracy: 0.9844 - 118ms/epoch - 20ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0194 - accuracy: 0.9961 - val_loss: 0.0368 - val_accuracy: 0.9844 - 120ms/epoch - 20ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0228 - accuracy: 0.9883 - val_loss: 0.0237 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 0.9844 - 147ms/epoch - 24ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.0443 - val_accuracy: 0.9844 - 182ms/epoch - 30ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0145 - accuracy: 0.9961 - val_loss: 0.0582 - val_accuracy: 0.9844 - 136ms/epoch - 23ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0137 - accuracy: 0.9961 - val_loss: 0.0581 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.0342 - val_accuracy: 0.9844 - 115ms/epoch - 19ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 0.9844 - 133ms/epoch - 22ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0457 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1351 - val_accuracy: 0.9688 - 127ms/epoch - 21ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0196 - accuracy: 0.9922 - val_loss: 0.2061 - val_accuracy: 0.9531 - 159ms/epoch - 26ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0390 - accuracy: 0.9883 - val_loss: 0.0960 - val_accuracy: 0.9844 - 143ms/epoch - 24ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0633 - val_accuracy: 0.9844 - 148ms/epoch - 25ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.9844 - 136ms/epoch - 23ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9844 - 223ms/epoch - 37ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 0.9844 - 225ms/epoch - 38ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 0.9961 - val_loss: 0.0607 - val_accuracy: 0.9844 - 166ms/epoch - 28ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0106 - accuracy: 0.9961 - val_loss: 0.0556 - val_accuracy: 0.9844 - 171ms/epoch - 29ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.9844 - 153ms/epoch - 25ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0143 - accuracy: 0.9922 - val_loss: 0.0518 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9844 - 165ms/epoch - 28ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9844 - 141ms/epoch - 24ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.0716 - val_accuracy: 0.9844 - 134ms/epoch - 22ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0099 - accuracy: 0.9961 - val_loss: 0.0660 - val_accuracy: 0.9844 - 141ms/epoch - 23ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9844 - 151ms/epoch - 25ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9844 - 150ms/epoch - 25ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0106 - accuracy: 0.9961 - val_loss: 0.0690 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9844 - 135ms/epoch - 22ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9844 - 156ms/epoch - 26ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9844 - 188ms/epoch - 31ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9844 - 168ms/epoch - 28ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9844 - 151ms/epoch - 25ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0139 - accuracy: 0.9922 - val_loss: 0.0659 - val_accuracy: 0.9844 - 177ms/epoch - 29ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9844 - 159ms/epoch - 27ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9844 - 149ms/epoch - 25ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0891 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0860 - val_accuracy: 0.9844 - 146ms/epoch - 24ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9844 - 141ms/epoch - 23ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.0692 - val_accuracy: 0.9844 - 159ms/epoch - 26ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0071 - accuracy: 0.9961 - val_loss: 0.0715 - val_accuracy: 0.9844 - 142ms/epoch - 24ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0166 - accuracy: 0.9922 - val_loss: 0.0758 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9844 - 149ms/epoch - 25ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9844 - 135ms/epoch - 23ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0596 - val_accuracy: 0.9844 - 135ms/epoch - 23ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9844 - 121ms/epoch - 20ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9844 - 129ms/epoch - 21ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0587 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9844 - 150ms/epoch - 25ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 8.4738e-04 - accuracy: 1.0000 - val_loss: 0.0599 - val_accuracy: 0.9844 - 129ms/epoch - 22ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0060 - accuracy: 0.9961 - val_loss: 0.0655 - val_accuracy: 0.9844 - 159ms/epoch - 27ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0669 - val_accuracy: 0.9844 - 160ms/epoch - 27ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 9.5906e-04 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9844 - 226ms/epoch - 38ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 9.0769e-04 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9844 - 165ms/epoch - 27ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 8.9593e-04 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9844 - 241ms/epoch - 40ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 9.9005e-04 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9844 - 202ms/epoch - 34ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9844 - 190ms/epoch - 32ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0062 - accuracy: 0.9961 - val_loss: 0.0399 - val_accuracy: 0.9844 - 185ms/epoch - 31ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 2s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 0.9844 - 2s/epoch - 258ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 2s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9844 - 2s/epoch - 275ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 2s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 0.9844 - 2s/epoch - 401ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 0.9844 - 184ms/epoch - 31ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 0.9844 - 190ms/epoch - 32ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 4s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 0.9844 - 4s/epoch - 728ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 9.1599e-04 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9844 - 169ms/epoch - 28ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9844 - 163ms/epoch - 27ms/step\n",
      "3/3 [==============================] - 4s 10ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "input_2 Quantized method: max-min  Values max: 25168.0 min: -23730.0 dec bit -8\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "conv1d_4 Quantized method: max-min  Values max: 24041.365 min: -17150.19 dec bit -8\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "batch_normalization_4 Quantized method: max-min  Values max: 6.930144 min: -6.10606 dec bit 4\n",
      "dropout_6 Quantized method: max-min  Values max: 6.930144 min: -6.10606 dec bit 4\n",
      "re_lu_5 Quantized method: max-min  Values max: 6.930144 min: -6.10606 dec bit 4\n",
      "max_pooling1d_4 Quantized method: max-min  Values max: 6.930144 min: -6.10606 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_5 Quantized method: max-min  Values max: 8.135707 min: -5.6247625 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_6 Quantized method: max-min  Values max: 7.9247627 min: -7.245602 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_5 Quantized method: max-min  Values max: 5.9218073 min: -5.516044 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_6 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "dropout_7 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "dropout_8 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "re_lu_6 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "re_lu_7 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "max_pooling1d_7 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "max_pooling1d_5 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "max_pooling1d_6 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "dropout_9 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "concatenate_1 Quantized method: max-min  Values max: 5.5386796 min: -6.0116982 dec bit 4\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "conv1d_7 Quantized method: max-min  Values max: 10.793967 min: -8.208349 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "batch_normalization_7 Quantized method: max-min  Values max: 5.142307 min: -3.9905322 dec bit 4\n",
      "re_lu_8 Quantized method: max-min  Values max: 5.142307 min: -3.9905322 dec bit 4\n",
      "dropout_10 Quantized method: max-min  Values max: 5.142307 min: -3.9905322 dec bit 4\n",
      "flatten_1 Quantized method: max-min  Values max: 5.142307 min: -3.9905322 dec bit 4\n",
      "3/3 [==============================] - 0s 19ms/step\n",
      "dense_2 Quantized method: max-min  Values max: 9.539857 min: -8.954737 dec bit 3\n",
      "dropout_11 Quantized method: max-min  Values max: 9.539857 min: -8.954737 dec bit 3\n",
      "re_lu_9 Quantized method: max-min  Values max: 9.539857 min: -8.954737 dec bit 3\n",
      "3/3 [==============================] - 1s 10ms/step\n",
      "dense_3 Quantized method: max-min  Values max: 16.205902 min: -13.352006 dec bit 2\n",
      "3/3 [==============================] - 0s 12ms/step\n",
      "softmax_1 Quantized method: max-min  Values max: 0.9999994 min: 7.087751e-13 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_1 : ['max_pooling1d_5', 'max_pooling1d_6', 'dropout_9']\n",
      "quantisation list {'input_2': [-8, 0], 'conv1d_4': [4, 0], 'batch_normalization_4': [4, 0], 'dropout_6': [4, 0], 're_lu_5': [4, 0], 'max_pooling1d_4': [4, 0], 'conv1d_5': [4, 0], 'conv1d_6': [4, 0], 'batch_normalization_5': [4, 0], 'batch_normalization_6': [4, 0], 'dropout_7': [4, 0], 'dropout_8': [4, 0], 're_lu_6': [4, 0], 're_lu_7': [4, 0], 'max_pooling1d_7': [4, 0], 'max_pooling1d_5': [4, 0], 'max_pooling1d_6': [4, 0], 'dropout_9': [4, 0], 'concatenate_1': [4, 0], 'conv1d_7': [4, 0], 'batch_normalization_7': [4, 0], 're_lu_8': [4, 0], 'dropout_10': [4, 0], 'flatten_1': [4, 0], 'dense_2': [3, 0], 'dropout_11': [3, 0], 're_lu_9': [3, 0], 'dense_3': [2, 0], 'softmax_1': [7, 0]}\n",
      "fusing batch normalization to conv1d_4\n",
      "original weight max 0.15342209 min -0.17054152\n",
      "original bias max 2.9506526e-07 min -3.4060872e-07\n",
      "fused weight max 0.00019039547 min -0.00015411832\n",
      "fused bias max 0.094518855 min -0.10178641\n",
      "quantizing weights for layer conv1d_4\n",
      "    tensor_conv1d_4_kernel_0 dec bit 19\n",
      "    tensor_conv1d_4_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_4\n",
      "fusing batch normalization to conv1d_5\n",
      "original weight max 0.17671214 min -0.16971634\n",
      "original bias max 0.00023258798 min -0.00014441779\n",
      "fused weight max 0.24758808 min -0.23361188\n",
      "fused bias max 1.2300969 min -1.0374281\n",
      "quantizing weights for layer conv1d_5\n",
      "    tensor_conv1d_5_kernel_0 dec bit 9\n",
      "    tensor_conv1d_5_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_6\n",
      "original weight max 0.20990758 min -0.21425857\n",
      "original bias max 0.00017650487 min -0.0001395839\n",
      "fused weight max 0.4217864 min -0.39004934\n",
      "fused bias max 1.0158854 min -0.8714352\n",
      "quantizing weights for layer conv1d_6\n",
      "    tensor_conv1d_6_kernel_0 dec bit 8\n",
      "    tensor_conv1d_6_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_5\n",
      "quantizing weights for layer batch_normalization_6\n",
      "fusing batch normalization to conv1d_7\n",
      "original weight max 0.16304567 min -0.17258957\n",
      "original bias max 0.00013854551 min -0.00014890212\n",
      "fused weight max 0.1463329 min -0.12817721\n",
      "fused bias max 1.5674524 min -1.0435659\n",
      "quantizing weights for layer conv1d_7\n",
      "    tensor_conv1d_7_kernel_0 dec bit 9\n",
      "    tensor_conv1d_7_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_7\n",
      "quantizing weights for layer dense_2\n",
      "    tensor_dense_2_kernel_0 dec bit 9\n",
      "    tensor_dense_2_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_3\n",
      "    tensor_dense_3_kernel_0 dec bit 8\n",
      "    tensor_dense_3_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 4s - loss: 1.6481 - accuracy: 0.4297 - val_loss: 1.3411 - val_accuracy: 0.5938 - 4s/epoch - 690ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.6864 - accuracy: 0.8633 - val_loss: 0.9070 - val_accuracy: 0.8438 - 158ms/epoch - 26ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.3222 - accuracy: 0.9258 - val_loss: 0.7519 - val_accuracy: 0.9062 - 172ms/epoch - 29ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.2069 - accuracy: 0.9531 - val_loss: 0.7560 - val_accuracy: 0.9219 - 167ms/epoch - 28ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.1472 - accuracy: 0.9648 - val_loss: 0.6857 - val_accuracy: 0.9375 - 163ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.0758 - accuracy: 0.9805 - val_loss: 0.5765 - val_accuracy: 0.9531 - 155ms/epoch - 26ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.0654 - accuracy: 0.9805 - val_loss: 0.4683 - val_accuracy: 0.9688 - 158ms/epoch - 26ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0410 - accuracy: 0.9922 - val_loss: 0.3923 - val_accuracy: 0.9531 - 151ms/epoch - 25ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0416 - accuracy: 0.9922 - val_loss: 0.3491 - val_accuracy: 0.9844 - 173ms/epoch - 29ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0296 - accuracy: 0.9961 - val_loss: 0.3458 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0375 - accuracy: 0.9961 - val_loss: 0.3325 - val_accuracy: 0.9844 - 182ms/epoch - 30ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0388 - accuracy: 0.9961 - val_loss: 0.3177 - val_accuracy: 0.9531 - 200ms/epoch - 33ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0262 - accuracy: 0.9961 - val_loss: 0.3066 - val_accuracy: 0.9688 - 167ms/epoch - 28ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0242 - accuracy: 0.9961 - val_loss: 0.2812 - val_accuracy: 0.9844 - 170ms/epoch - 28ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0230 - accuracy: 0.9961 - val_loss: 0.2600 - val_accuracy: 0.9844 - 154ms/epoch - 26ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.2448 - val_accuracy: 0.9688 - 178ms/epoch - 30ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0203 - accuracy: 0.9922 - val_loss: 0.2547 - val_accuracy: 0.9688 - 170ms/epoch - 28ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0147 - accuracy: 0.9961 - val_loss: 0.2645 - val_accuracy: 0.9688 - 179ms/epoch - 30ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2591 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.2437 - val_accuracy: 0.9688 - 155ms/epoch - 26ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9688 - 152ms/epoch - 25ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0248 - accuracy: 0.9961 - val_loss: 0.2915 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 0.9688 - 146ms/epoch - 24ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9688 - 142ms/epoch - 24ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0077 - accuracy: 0.9961 - val_loss: 0.2443 - val_accuracy: 0.9688 - 139ms/epoch - 23ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2459 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9688 - 143ms/epoch - 24ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.2270 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.2398 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2609 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2916 - val_accuracy: 0.9688 - 143ms/epoch - 24ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.2971 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.9531 - 137ms/epoch - 23ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 0.9531 - 143ms/epoch - 24ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9531 - 131ms/epoch - 22ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2918 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2846 - val_accuracy: 0.9531 - 166ms/epoch - 28ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2733 - val_accuracy: 0.9531 - 159ms/epoch - 27ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2605 - val_accuracy: 0.9531 - 132ms/epoch - 22ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2371 - val_accuracy: 0.9531 - 164ms/epoch - 27ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2190 - val_accuracy: 0.9531 - 153ms/epoch - 25ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2083 - val_accuracy: 0.9531 - 151ms/epoch - 25ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.2063 - val_accuracy: 0.9531 - 156ms/epoch - 26ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2073 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2094 - val_accuracy: 0.9688 - 163ms/epoch - 27ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2069 - val_accuracy: 0.9688 - 161ms/epoch - 27ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2024 - val_accuracy: 0.9688 - 159ms/epoch - 27ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2027 - val_accuracy: 0.9688 - 145ms/epoch - 24ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2037 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2057 - val_accuracy: 0.9688 - 147ms/epoch - 25ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9688 - 149ms/epoch - 25ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9688 - 141ms/epoch - 23ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2493 - val_accuracy: 0.9688 - 151ms/epoch - 25ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2169 - val_accuracy: 0.9688 - 152ms/epoch - 25ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9688 - 139ms/epoch - 23ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9688 - 169ms/epoch - 28ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1768 - val_accuracy: 0.9688 - 122ms/epoch - 20ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1925 - val_accuracy: 0.9688 - 141ms/epoch - 24ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0097 - accuracy: 0.9961 - val_loss: 0.2211 - val_accuracy: 0.9688 - 183ms/epoch - 30ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3009 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0406 - accuracy: 0.9844 - val_loss: 0.3157 - val_accuracy: 0.9688 - 154ms/epoch - 26ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3077 - val_accuracy: 0.9531 - 156ms/epoch - 26ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.3003 - val_accuracy: 0.9531 - 143ms/epoch - 24ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.3121 - val_accuracy: 0.9531 - 191ms/epoch - 32ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 0.9531 - 153ms/epoch - 26ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.3085 - val_accuracy: 0.9531 - 145ms/epoch - 24ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9531 - 141ms/epoch - 24ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2398 - val_accuracy: 0.9844 - 162ms/epoch - 27ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9844 - 150ms/epoch - 25ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2279 - val_accuracy: 0.9688 - 181ms/epoch - 30ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 8.5922e-04 - accuracy: 1.0000 - val_loss: 0.2354 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2362 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9688 - 176ms/epoch - 29ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0085 - accuracy: 0.9961 - val_loss: 0.2063 - val_accuracy: 0.9688 - 164ms/epoch - 27ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 7.5934e-04 - accuracy: 1.0000 - val_loss: 0.1966 - val_accuracy: 0.9688 - 177ms/epoch - 30ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1954 - val_accuracy: 0.9688 - 174ms/epoch - 29ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2014 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2100 - val_accuracy: 0.9688 - 141ms/epoch - 23ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2179 - val_accuracy: 0.9688 - 137ms/epoch - 23ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2134 - val_accuracy: 0.9688 - 141ms/epoch - 24ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 0.9961 - val_loss: 0.2094 - val_accuracy: 0.9688 - 153ms/epoch - 25ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2096 - val_accuracy: 0.9844 - 138ms/epoch - 23ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2133 - val_accuracy: 0.9844 - 148ms/epoch - 25ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.2188 - val_accuracy: 0.9844 - 169ms/epoch - 28ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 9.3348e-04 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 0.9688 - 145ms/epoch - 24ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 6.1154e-04 - accuracy: 1.0000 - val_loss: 0.2685 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2713 - val_accuracy: 0.9688 - 163ms/epoch - 27ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9688 - 192ms/epoch - 32ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 8.5469e-04 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.9688 - 177ms/epoch - 30ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 3.6522e-04 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9688 - 133ms/epoch - 22ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 8.4616e-04 - accuracy: 1.0000 - val_loss: 0.2455 - val_accuracy: 0.9688 - 158ms/epoch - 26ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 3.3645e-04 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.9688 - 174ms/epoch - 29ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2338 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2367 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9688 - 164ms/epoch - 27ms/step\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1613 - accuracy: 0.9750\n",
      "input_3 Quantized method: max-min  Values max: 32767.0 min: -20393.0 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_8 Quantized method: max-min  Values max: 12891.828 min: -12726.757 dec bit -7\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_8 Quantized method: max-min  Values max: 8.048 min: -8.960676 dec bit 3\n",
      "dropout_12 Quantized method: max-min  Values max: 8.048 min: -8.960676 dec bit 3\n",
      "re_lu_10 Quantized method: max-min  Values max: 8.048 min: -8.960676 dec bit 3\n",
      "max_pooling1d_8 Quantized method: max-min  Values max: 8.048 min: -8.960676 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_9 Quantized method: max-min  Values max: 6.580804 min: -7.002034 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_10 Quantized method: max-min  Values max: 5.152588 min: -6.462636 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_9 Quantized method: max-min  Values max: 6.3479304 min: -6.800978 dec bit 4\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "batch_normalization_10 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "dropout_13 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "dropout_14 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "re_lu_11 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "re_lu_12 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "max_pooling1d_11 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "max_pooling1d_9 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "max_pooling1d_10 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "dropout_15 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "concatenate_2 Quantized method: max-min  Values max: 7.005711 min: -6.785303 dec bit 4\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "conv1d_11 Quantized method: max-min  Values max: 7.84098 min: -8.346189 dec bit 3\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "batch_normalization_11 Quantized method: max-min  Values max: 5.0093894 min: -5.269288 dec bit 4\n",
      "re_lu_13 Quantized method: max-min  Values max: 5.0093894 min: -5.269288 dec bit 4\n",
      "dropout_16 Quantized method: max-min  Values max: 5.0093894 min: -5.269288 dec bit 4\n",
      "flatten_2 Quantized method: max-min  Values max: 5.0093894 min: -5.269288 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_4 Quantized method: max-min  Values max: 8.5628195 min: -6.3512983 dec bit 3\n",
      "dropout_17 Quantized method: max-min  Values max: 8.5628195 min: -6.3512983 dec bit 3\n",
      "re_lu_14 Quantized method: max-min  Values max: 8.5628195 min: -6.3512983 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_5 Quantized method: max-min  Values max: 19.020252 min: -9.720228 dec bit 2\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "softmax_2 Quantized method: max-min  Values max: 0.9999999 min: 3.297372e-13 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_2 : ['max_pooling1d_9', 'max_pooling1d_10', 'dropout_15']\n",
      "quantisation list {'input_3': [-8, 0], 'conv1d_8': [4, 0], 'batch_normalization_8': [4, 0], 'dropout_12': [4, 0], 're_lu_10': [4, 0], 'max_pooling1d_8': [4, 0], 'conv1d_9': [4, 0], 'conv1d_10': [4, 0], 'batch_normalization_9': [4, 0], 'batch_normalization_10': [4, 0], 'dropout_13': [4, 0], 'dropout_14': [4, 0], 're_lu_11': [4, 0], 're_lu_12': [4, 0], 'max_pooling1d_11': [4, 0], 'max_pooling1d_9': [4, 0], 'max_pooling1d_10': [4, 0], 'dropout_15': [4, 0], 'concatenate_2': [4, 0], 'conv1d_11': [4, 0], 'batch_normalization_11': [4, 0], 're_lu_13': [4, 0], 'dropout_16': [4, 0], 'flatten_2': [4, 0], 'dense_4': [3, 0], 'dropout_17': [3, 0], 're_lu_14': [3, 0], 'dense_5': [2, 0], 'softmax_2': [7, 0]}\n",
      "fusing batch normalization to conv1d_8\n",
      "original weight max 0.1594067 min -0.15489513\n",
      "original bias max 1.4912014e-07 min -3.183789e-07\n",
      "fused weight max 0.00019225861 min -0.00020264671\n",
      "fused bias max 0.070344076 min -0.083995335\n",
      "quantizing weights for layer conv1d_8\n",
      "    tensor_conv1d_8_kernel_0 dec bit 19\n",
      "    tensor_conv1d_8_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_8\n",
      "fusing batch normalization to conv1d_9\n",
      "original weight max 0.16592751 min -0.17095028\n",
      "original bias max 0.00016350513 min -0.00016339993\n",
      "fused weight max 0.23568787 min -0.24426135\n",
      "fused bias max 1.1519971 min -0.9614196\n",
      "quantizing weights for layer conv1d_9\n",
      "    tensor_conv1d_9_kernel_0 dec bit 9\n",
      "    tensor_conv1d_9_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_10\n",
      "original weight max 0.20341107 min -0.21404856\n",
      "original bias max 0.00017646149 min -0.0002836649\n",
      "fused weight max 0.3150508 min -0.34077272\n",
      "fused bias max 1.0182083 min -0.6335263\n",
      "quantizing weights for layer conv1d_10\n",
      "    tensor_conv1d_10_kernel_0 dec bit 8\n",
      "    tensor_conv1d_10_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_9\n",
      "quantizing weights for layer batch_normalization_10\n",
      "fusing batch normalization to conv1d_11\n",
      "original weight max 0.15893723 min -0.15976731\n",
      "original bias max 0.00028671158 min -0.00014582694\n",
      "fused weight max 0.15401733 min -0.15550567\n",
      "fused bias max 1.1992599 min -1.0536062\n",
      "quantizing weights for layer conv1d_11\n",
      "    tensor_conv1d_11_kernel_0 dec bit 9\n",
      "    tensor_conv1d_11_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_11\n",
      "quantizing weights for layer dense_4\n",
      "    tensor_dense_4_kernel_0 dec bit 9\n",
      "    tensor_dense_4_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_5\n",
      "    tensor_dense_5_kernel_0 dec bit 8\n",
      "    tensor_dense_5_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 5s - loss: 1.7949 - accuracy: 0.3789 - val_loss: 1.5838 - val_accuracy: 0.5156 - 5s/epoch - 798ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.6960 - accuracy: 0.8164 - val_loss: 1.0013 - val_accuracy: 0.7344 - 153ms/epoch - 25ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.3130 - accuracy: 0.9297 - val_loss: 1.0496 - val_accuracy: 0.8906 - 159ms/epoch - 27ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.2059 - accuracy: 0.9414 - val_loss: 1.1156 - val_accuracy: 0.8750 - 149ms/epoch - 25ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.1459 - accuracy: 0.9648 - val_loss: 1.1432 - val_accuracy: 0.8438 - 164ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1072 - accuracy: 0.9766 - val_loss: 1.1017 - val_accuracy: 0.8594 - 186ms/epoch - 31ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1031 - accuracy: 0.9688 - val_loss: 0.9266 - val_accuracy: 0.8594 - 144ms/epoch - 24ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0615 - accuracy: 0.9883 - val_loss: 0.7969 - val_accuracy: 0.8750 - 150ms/epoch - 25ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0714 - accuracy: 0.9766 - val_loss: 0.7218 - val_accuracy: 0.8906 - 138ms/epoch - 23ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0412 - accuracy: 0.9961 - val_loss: 0.6924 - val_accuracy: 0.8906 - 132ms/epoch - 22ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0483 - accuracy: 0.9883 - val_loss: 0.6393 - val_accuracy: 0.9062 - 130ms/epoch - 22ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0342 - accuracy: 0.9922 - val_loss: 0.6178 - val_accuracy: 0.9219 - 117ms/epoch - 19ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0422 - accuracy: 0.9883 - val_loss: 0.5765 - val_accuracy: 0.9375 - 139ms/epoch - 23ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0308 - accuracy: 0.9922 - val_loss: 0.5443 - val_accuracy: 0.9531 - 138ms/epoch - 23ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.5325 - val_accuracy: 0.9531 - 132ms/epoch - 22ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.5140 - val_accuracy: 0.9531 - 148ms/epoch - 25ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0213 - accuracy: 0.9961 - val_loss: 0.5062 - val_accuracy: 0.9531 - 150ms/epoch - 25ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.4875 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.4625 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.9531 - 152ms/epoch - 25ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.9531 - 192ms/epoch - 32ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.4146 - val_accuracy: 0.9531 - 147ms/epoch - 24ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0176 - accuracy: 0.9961 - val_loss: 0.3923 - val_accuracy: 0.9531 - 151ms/epoch - 25ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.3708 - val_accuracy: 0.9531 - 141ms/epoch - 24ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0141 - accuracy: 0.9961 - val_loss: 0.3625 - val_accuracy: 0.9531 - 154ms/epoch - 26ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3692 - val_accuracy: 0.9531 - 128ms/epoch - 21ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0240 - accuracy: 0.9883 - val_loss: 0.4218 - val_accuracy: 0.9531 - 131ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.4203 - val_accuracy: 0.9531 - 141ms/epoch - 23ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0169 - accuracy: 0.9922 - val_loss: 0.3939 - val_accuracy: 0.9531 - 118ms/epoch - 20ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.3875 - val_accuracy: 0.9688 - 141ms/epoch - 23ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0409 - accuracy: 0.9844 - val_loss: 0.3846 - val_accuracy: 0.9688 - 112ms/epoch - 19ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0200 - accuracy: 0.9961 - val_loss: 0.3526 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3240 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0147 - accuracy: 0.9922 - val_loss: 0.3134 - val_accuracy: 0.9688 - 156ms/epoch - 26ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0091 - accuracy: 0.9961 - val_loss: 0.3094 - val_accuracy: 0.9688 - 146ms/epoch - 24ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0104 - accuracy: 0.9961 - val_loss: 0.3063 - val_accuracy: 0.9688 - 156ms/epoch - 26ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3069 - val_accuracy: 0.9688 - 173ms/epoch - 29ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0089 - accuracy: 0.9961 - val_loss: 0.3086 - val_accuracy: 0.9688 - 122ms/epoch - 20ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3115 - val_accuracy: 0.9688 - 136ms/epoch - 23ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0060 - accuracy: 0.9961 - val_loss: 0.3072 - val_accuracy: 0.9688 - 149ms/epoch - 25ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9531 - 157ms/epoch - 26ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2934 - val_accuracy: 0.9531 - 147ms/epoch - 25ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2929 - val_accuracy: 0.9531 - 128ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2936 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2950 - val_accuracy: 0.9531 - 123ms/epoch - 20ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2934 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2918 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 0.9961 - val_loss: 0.2902 - val_accuracy: 0.9531 - 123ms/epoch - 21ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2851 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2865 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2876 - val_accuracy: 0.9531 - 159ms/epoch - 27ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 0.9688 - 147ms/epoch - 25ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2865 - val_accuracy: 0.9688 - 133ms/epoch - 22ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2897 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2932 - val_accuracy: 0.9531 - 132ms/epoch - 22ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3118 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0067 - accuracy: 0.9961 - val_loss: 0.3160 - val_accuracy: 0.9531 - 142ms/epoch - 24ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3194 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3243 - val_accuracy: 0.9531 - 126ms/epoch - 21ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3294 - val_accuracy: 0.9531 - 128ms/epoch - 21ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9531 - 117ms/epoch - 20ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3416 - val_accuracy: 0.9531 - 115ms/epoch - 19ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.3448 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3399 - val_accuracy: 0.9531 - 117ms/epoch - 19ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.3324 - val_accuracy: 0.9375 - 119ms/epoch - 20ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3311 - val_accuracy: 0.9375 - 122ms/epoch - 20ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0092 - accuracy: 0.9961 - val_loss: 0.3247 - val_accuracy: 0.9375 - 158ms/epoch - 26ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3264 - val_accuracy: 0.9375 - 153ms/epoch - 26ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.3318 - val_accuracy: 0.9375 - 133ms/epoch - 22ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3389 - val_accuracy: 0.9375 - 128ms/epoch - 21ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3241 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3207 - val_accuracy: 0.9531 - 148ms/epoch - 25ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 8.5344e-04 - accuracy: 1.0000 - val_loss: 0.3196 - val_accuracy: 0.9531 - 137ms/epoch - 23ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 0.9531 - 160ms/epoch - 27ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3174 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.9531 - 129ms/epoch - 21ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 7.6153e-04 - accuracy: 1.0000 - val_loss: 0.3212 - val_accuracy: 0.9531 - 130ms/epoch - 22ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 7.2951e-04 - accuracy: 1.0000 - val_loss: 0.3200 - val_accuracy: 0.9531 - 137ms/epoch - 23ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3240 - val_accuracy: 0.9531 - 135ms/epoch - 23ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0044 - accuracy: 0.9961 - val_loss: 0.3328 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 9.8515e-04 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9531 - 128ms/epoch - 21ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 9.4974e-04 - accuracy: 1.0000 - val_loss: 0.3396 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 6.1848e-04 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9531 - 154ms/epoch - 26ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3407 - val_accuracy: 0.9531 - 138ms/epoch - 23ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3348 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 9.2555e-04 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 0.9531 - 144ms/epoch - 24ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 0.9961 - val_loss: 0.3455 - val_accuracy: 0.9375 - 161ms/epoch - 27ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.0070 - accuracy: 0.9961 - val_loss: 0.4836 - val_accuracy: 0.9375 - 138ms/epoch - 23ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0543 - accuracy: 0.9766 - val_loss: 0.4354 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.5848 - val_accuracy: 0.9219 - 129ms/epoch - 21ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0674 - accuracy: 0.9727 - val_loss: 0.5011 - val_accuracy: 0.9219 - 119ms/epoch - 20ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.9219 - 127ms/epoch - 21ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.5239 - val_accuracy: 0.9062 - 120ms/epoch - 20ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0165 - accuracy: 0.9922 - val_loss: 0.5157 - val_accuracy: 0.9219 - 122ms/epoch - 20ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5311 - val_accuracy: 0.9219 - 138ms/epoch - 23ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.9219 - 149ms/epoch - 25ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.9219 - 134ms/epoch - 22ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5105 - val_accuracy: 0.9375 - 136ms/epoch - 23ms/step\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0858 - accuracy: 0.9750\n",
      "input_4 Quantized method: max-min  Values max: 23323.0 min: -23160.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d_12 Quantized method: max-min  Values max: 18681.016 min: -18342.09 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_12 Quantized method: max-min  Values max: 6.7199345 min: -6.336118 dec bit 4\n",
      "dropout_18 Quantized method: max-min  Values max: 6.7199345 min: -6.336118 dec bit 4\n",
      "re_lu_15 Quantized method: max-min  Values max: 6.7199345 min: -6.336118 dec bit 4\n",
      "max_pooling1d_12 Quantized method: max-min  Values max: 6.7199345 min: -6.336118 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_13 Quantized method: max-min  Values max: 7.6389284 min: -6.0616045 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_14 Quantized method: max-min  Values max: 6.3462334 min: -6.1757264 dec bit 4\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "batch_normalization_13 Quantized method: max-min  Values max: 5.743083 min: -4.936261 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_14 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "dropout_19 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "dropout_20 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "re_lu_16 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "re_lu_17 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "max_pooling1d_15 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "max_pooling1d_13 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "max_pooling1d_14 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "dropout_21 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "concatenate_3 Quantized method: max-min  Values max: 6.024736 min: -6.7508454 dec bit 4\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "conv1d_15 Quantized method: max-min  Values max: 9.193981 min: -7.1560173 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_15 Quantized method: max-min  Values max: 5.268757 min: -4.502453 dec bit 4\n",
      "re_lu_18 Quantized method: max-min  Values max: 5.268757 min: -4.502453 dec bit 4\n",
      "dropout_22 Quantized method: max-min  Values max: 5.268757 min: -4.502453 dec bit 4\n",
      "flatten_3 Quantized method: max-min  Values max: 5.268757 min: -4.502453 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_6 Quantized method: max-min  Values max: 12.984327 min: -12.116119 dec bit 3\n",
      "dropout_23 Quantized method: max-min  Values max: 12.984327 min: -12.116119 dec bit 3\n",
      "re_lu_19 Quantized method: max-min  Values max: 12.984327 min: -12.116119 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_7 Quantized method: max-min  Values max: 22.030788 min: -11.797422 dec bit 2\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "softmax_3 Quantized method: max-min  Values max: 1.0 min: 3.3007092e-14 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_3 : ['max_pooling1d_13', 'max_pooling1d_14', 'dropout_21']\n",
      "quantisation list {'input_4': [-8, 0], 'conv1d_12': [4, 0], 'batch_normalization_12': [4, 0], 'dropout_18': [4, 0], 're_lu_15': [4, 0], 'max_pooling1d_12': [4, 0], 'conv1d_13': [4, 0], 'conv1d_14': [4, 0], 'batch_normalization_13': [4, 0], 'batch_normalization_14': [4, 0], 'dropout_19': [4, 0], 'dropout_20': [4, 0], 're_lu_16': [4, 0], 're_lu_17': [4, 0], 'max_pooling1d_15': [4, 0], 'max_pooling1d_13': [4, 0], 'max_pooling1d_14': [4, 0], 'dropout_21': [4, 0], 'concatenate_3': [4, 0], 'conv1d_15': [4, 0], 'batch_normalization_15': [4, 0], 're_lu_18': [4, 0], 'dropout_22': [4, 0], 'flatten_3': [4, 0], 'dense_6': [3, 0], 'dropout_23': [3, 0], 're_lu_19': [3, 0], 'dense_7': [2, 0], 'softmax_3': [7, 0]}\n",
      "fusing batch normalization to conv1d_12\n",
      "original weight max 0.15303853 min -0.16282122\n",
      "original bias max 1.1255215e-06 min -4.2724452e-07\n",
      "fused weight max 0.00017502422 min -0.00014192927\n",
      "fused bias max 0.07384772 min -0.105751574\n",
      "quantizing weights for layer conv1d_12\n",
      "    tensor_conv1d_12_kernel_0 dec bit 19\n",
      "    tensor_conv1d_12_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_12\n",
      "fusing batch normalization to conv1d_13\n",
      "original weight max 0.17495118 min -0.16235323\n",
      "original bias max 0.00019915558 min -0.00016416915\n",
      "fused weight max 0.2984624 min -0.3146834\n",
      "fused bias max 0.99289054 min -0.78176415\n",
      "quantizing weights for layer conv1d_13\n",
      "    tensor_conv1d_13_kernel_0 dec bit 8\n",
      "    tensor_conv1d_13_bias_0 dec bit 7\n",
      "fusing batch normalization to conv1d_14\n",
      "original weight max 0.20838185 min -0.206238\n",
      "original bias max 0.00011129121 min -0.00020490549\n",
      "fused weight max 0.35028002 min -0.37146634\n",
      "fused bias max 1.0701102 min -0.64195323\n",
      "quantizing weights for layer conv1d_14\n",
      "    tensor_conv1d_14_kernel_0 dec bit 8\n",
      "    tensor_conv1d_14_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_13\n",
      "quantizing weights for layer batch_normalization_14\n",
      "fusing batch normalization to conv1d_15\n",
      "original weight max 0.17581618 min -0.16979937\n",
      "original bias max 0.00018187276 min -0.0001773536\n",
      "fused weight max 0.15789483 min -0.1511867\n",
      "fused bias max 1.1757156 min -0.8975534\n",
      "quantizing weights for layer conv1d_15\n",
      "    tensor_conv1d_15_kernel_0 dec bit 9\n",
      "    tensor_conv1d_15_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_15\n",
      "quantizing weights for layer dense_6\n",
      "    tensor_dense_6_kernel_0 dec bit 9\n",
      "    tensor_dense_6_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_7\n",
      "    tensor_dense_7_kernel_0 dec bit 8\n",
      "    tensor_dense_7_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 1.8305 - accuracy: 0.3555 - val_loss: 1.5074 - val_accuracy: 0.6406 - 3s/epoch - 504ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.8315 - accuracy: 0.8164 - val_loss: 1.5400 - val_accuracy: 0.6719 - 179ms/epoch - 30ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.4478 - accuracy: 0.8828 - val_loss: 1.3749 - val_accuracy: 0.8438 - 177ms/epoch - 30ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.2848 - accuracy: 0.9297 - val_loss: 1.1127 - val_accuracy: 0.8750 - 159ms/epoch - 26ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2067 - accuracy: 0.9414 - val_loss: 0.9492 - val_accuracy: 0.9062 - 159ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1652 - accuracy: 0.9492 - val_loss: 0.8850 - val_accuracy: 0.8906 - 160ms/epoch - 27ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1077 - accuracy: 0.9727 - val_loss: 0.8661 - val_accuracy: 0.8750 - 133ms/epoch - 22ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.1039 - accuracy: 0.9688 - val_loss: 0.7713 - val_accuracy: 0.9375 - 144ms/epoch - 24ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0899 - accuracy: 0.9844 - val_loss: 0.6473 - val_accuracy: 0.9219 - 144ms/epoch - 24ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0731 - accuracy: 0.9766 - val_loss: 0.5644 - val_accuracy: 0.9375 - 169ms/epoch - 28ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0423 - accuracy: 0.9922 - val_loss: 0.4968 - val_accuracy: 0.9375 - 151ms/epoch - 25ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0449 - accuracy: 0.9961 - val_loss: 0.4681 - val_accuracy: 0.9375 - 143ms/epoch - 24ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0448 - accuracy: 0.9883 - val_loss: 0.4707 - val_accuracy: 0.9375 - 131ms/epoch - 22ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0301 - accuracy: 0.9922 - val_loss: 0.5493 - val_accuracy: 0.9062 - 161ms/epoch - 27ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0413 - accuracy: 0.9922 - val_loss: 0.5072 - val_accuracy: 0.9375 - 121ms/epoch - 20ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0251 - accuracy: 0.9961 - val_loss: 0.4317 - val_accuracy: 0.9688 - 176ms/epoch - 29ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0447 - accuracy: 0.9805 - val_loss: 0.4042 - val_accuracy: 0.9688 - 174ms/epoch - 29ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0341 - accuracy: 0.9961 - val_loss: 0.3722 - val_accuracy: 0.9688 - 140ms/epoch - 23ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0321 - accuracy: 0.9922 - val_loss: 0.3949 - val_accuracy: 0.9531 - 135ms/epoch - 22ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0257 - accuracy: 0.9961 - val_loss: 0.3890 - val_accuracy: 0.9688 - 168ms/epoch - 28ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 0.9844 - 149ms/epoch - 25ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.3464 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0157 - accuracy: 0.9922 - val_loss: 0.3430 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0229 - accuracy: 0.9961 - val_loss: 0.3304 - val_accuracy: 0.9844 - 154ms/epoch - 26ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0229 - accuracy: 0.9922 - val_loss: 0.3250 - val_accuracy: 0.9844 - 123ms/epoch - 21ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.3297 - val_accuracy: 0.9844 - 118ms/epoch - 20ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0364 - accuracy: 0.9883 - val_loss: 0.3344 - val_accuracy: 0.9844 - 151ms/epoch - 25ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0172 - accuracy: 0.9961 - val_loss: 0.3321 - val_accuracy: 0.9844 - 123ms/epoch - 20ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.3226 - val_accuracy: 0.9844 - 154ms/epoch - 26ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0153 - accuracy: 0.9961 - val_loss: 0.3180 - val_accuracy: 0.9844 - 131ms/epoch - 22ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.3151 - val_accuracy: 0.9844 - 122ms/epoch - 20ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3074 - val_accuracy: 0.9844 - 163ms/epoch - 27ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3041 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.3015 - val_accuracy: 0.9844 - 165ms/epoch - 27ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.2910 - val_accuracy: 0.9844 - 162ms/epoch - 27ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2765 - val_accuracy: 0.9844 - 149ms/epoch - 25ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.2656 - val_accuracy: 0.9844 - 141ms/epoch - 23ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.2616 - val_accuracy: 0.9844 - 160ms/epoch - 27ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2619 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2629 - val_accuracy: 0.9844 - 147ms/epoch - 25ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.2664 - val_accuracy: 0.9844 - 139ms/epoch - 23ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0191 - accuracy: 0.9961 - val_loss: 0.2849 - val_accuracy: 0.9844 - 135ms/epoch - 22ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0206 - accuracy: 0.9922 - val_loss: 0.3191 - val_accuracy: 0.9844 - 157ms/epoch - 26ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.3520 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0579 - accuracy: 0.9883 - val_loss: 0.3683 - val_accuracy: 0.9844 - 117ms/epoch - 20ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0347 - accuracy: 0.9844 - val_loss: 0.4085 - val_accuracy: 0.9844 - 117ms/epoch - 20ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0415 - accuracy: 0.9883 - val_loss: 0.4038 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0310 - accuracy: 0.9961 - val_loss: 0.3505 - val_accuracy: 0.9844 - 118ms/epoch - 20ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0153 - accuracy: 0.9961 - val_loss: 0.2996 - val_accuracy: 0.9844 - 119ms/epoch - 20ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0095 - accuracy: 0.9961 - val_loss: 0.2700 - val_accuracy: 0.9844 - 123ms/epoch - 20ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9844 - 123ms/epoch - 20ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0099 - accuracy: 0.9961 - val_loss: 0.2497 - val_accuracy: 0.9844 - 134ms/epoch - 22ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0157 - accuracy: 0.9922 - val_loss: 0.2498 - val_accuracy: 0.9844 - 161ms/epoch - 27ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2543 - val_accuracy: 0.9844 - 141ms/epoch - 24ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2559 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2550 - val_accuracy: 0.9844 - 149ms/epoch - 25ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2501 - val_accuracy: 0.9844 - 155ms/epoch - 26ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9844 - 147ms/epoch - 25ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2444 - val_accuracy: 0.9844 - 115ms/epoch - 19ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2443 - val_accuracy: 0.9844 - 150ms/epoch - 25ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.9844 - 120ms/epoch - 20ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.2559 - val_accuracy: 0.9844 - 144ms/epoch - 24ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2576 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2578 - val_accuracy: 0.9844 - 135ms/epoch - 23ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9844 - 146ms/epoch - 24ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2585 - val_accuracy: 0.9844 - 149ms/epoch - 25ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2540 - val_accuracy: 0.9844 - 135ms/epoch - 22ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2517 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2512 - val_accuracy: 0.9844 - 132ms/epoch - 22ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0063 - accuracy: 0.9961 - val_loss: 0.2489 - val_accuracy: 0.9844 - 147ms/epoch - 25ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 0.9844 - 121ms/epoch - 20ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0081 - accuracy: 0.9961 - val_loss: 0.2255 - val_accuracy: 0.9844 - 135ms/epoch - 22ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2040 - val_accuracy: 0.9844 - 140ms/epoch - 23ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0209 - accuracy: 0.9922 - val_loss: 0.2143 - val_accuracy: 0.9844 - 145ms/epoch - 24ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0157 - accuracy: 0.9922 - val_loss: 0.2117 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0401 - accuracy: 0.9922 - val_loss: 0.2263 - val_accuracy: 0.9844 - 116ms/epoch - 19ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0081 - accuracy: 0.9961 - val_loss: 0.2328 - val_accuracy: 0.9844 - 161ms/epoch - 27ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0139 - accuracy: 0.9922 - val_loss: 0.2316 - val_accuracy: 0.9844 - 133ms/epoch - 22ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2257 - val_accuracy: 0.9844 - 122ms/epoch - 20ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0116 - accuracy: 0.9922 - val_loss: 0.2220 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.2232 - val_accuracy: 0.9844 - 163ms/epoch - 27ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2636 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0208 - accuracy: 0.9961 - val_loss: 0.2363 - val_accuracy: 0.9688 - 143ms/epoch - 24ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9844 - 156ms/epoch - 26ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.9844 - 157ms/epoch - 26ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2261 - val_accuracy: 0.9844 - 148ms/epoch - 25ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2248 - val_accuracy: 0.9844 - 133ms/epoch - 22ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 0.9844 - 125ms/epoch - 21ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2210 - val_accuracy: 0.9844 - 141ms/epoch - 23ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2223 - val_accuracy: 0.9844 - 153ms/epoch - 25ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2228 - val_accuracy: 0.9844 - 116ms/epoch - 19ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2226 - val_accuracy: 0.9844 - 136ms/epoch - 23ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9844 - 127ms/epoch - 21ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2209 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0185 - accuracy: 0.9961 - val_loss: 0.2230 - val_accuracy: 0.9844 - 129ms/epoch - 22ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0188 - accuracy: 0.9922 - val_loss: 0.2660 - val_accuracy: 0.9844 - 144ms/epoch - 24ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0435 - accuracy: 0.9805 - val_loss: 0.2506 - val_accuracy: 0.9844 - 137ms/epoch - 23ms/step\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "input_5 Quantized method: max-min  Values max: 32767.0 min: -28326.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d_16 Quantized method: max-min  Values max: 14526.727 min: -26351.166 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_16 Quantized method: max-min  Values max: 7.020069 min: -13.5574 dec bit 3\n",
      "dropout_24 Quantized method: max-min  Values max: 7.020069 min: -13.5574 dec bit 3\n",
      "re_lu_20 Quantized method: max-min  Values max: 7.020069 min: -13.5574 dec bit 3\n",
      "max_pooling1d_16 Quantized method: max-min  Values max: 7.020069 min: -13.5574 dec bit 3\n",
      "3/3 [==============================] - 0s 15ms/step\n",
      "conv1d_17 Quantized method: max-min  Values max: 8.052411 min: -7.773214 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_18 Quantized method: max-min  Values max: 8.046656 min: -8.907354 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_17 Quantized method: max-min  Values max: 6.151306 min: -7.9630733 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_18 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "dropout_25 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "dropout_26 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "re_lu_21 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "re_lu_22 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "max_pooling1d_19 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "max_pooling1d_17 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "max_pooling1d_18 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "dropout_27 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "concatenate_4 Quantized method: max-min  Values max: 7.182866 min: -8.055553 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_19 Quantized method: max-min  Values max: 11.77475 min: -8.2626 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_19 Quantized method: max-min  Values max: 5.420205 min: -5.032205 dec bit 4\n",
      "re_lu_23 Quantized method: max-min  Values max: 5.420205 min: -5.032205 dec bit 4\n",
      "dropout_28 Quantized method: max-min  Values max: 5.420205 min: -5.032205 dec bit 4\n",
      "flatten_4 Quantized method: max-min  Values max: 5.420205 min: -5.032205 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_8 Quantized method: max-min  Values max: 9.2562475 min: -7.320119 dec bit 3\n",
      "dropout_29 Quantized method: max-min  Values max: 9.2562475 min: -7.320119 dec bit 3\n",
      "re_lu_24 Quantized method: max-min  Values max: 9.2562475 min: -7.320119 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_9 Quantized method: max-min  Values max: 15.210554 min: -10.556878 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "softmax_4 Quantized method: max-min  Values max: 0.99999917 min: 7.1378625e-11 dec bit 7\n",
      "set dec bit 3 for the input of concatenate_4 : ['max_pooling1d_17', 'max_pooling1d_18', 'dropout_27']\n",
      "quantisation list {'input_5': [-8, 0], 'conv1d_16': [3, 0], 'batch_normalization_16': [3, 0], 'dropout_24': [3, 0], 're_lu_20': [3, 0], 'max_pooling1d_16': [3, 0], 'conv1d_17': [3, 0], 'conv1d_18': [3, 0], 'batch_normalization_17': [3, 0], 'batch_normalization_18': [3, 0], 'dropout_25': [3, 0], 'dropout_26': [3, 0], 're_lu_21': [3, 0], 're_lu_22': [3, 0], 'max_pooling1d_19': [3, 0], 'max_pooling1d_17': [3, 0], 'max_pooling1d_18': [3, 0], 'dropout_27': [3, 0], 'concatenate_4': [3, 0], 'conv1d_19': [4, 0], 'batch_normalization_19': [4, 0], 're_lu_23': [4, 0], 'dropout_28': [4, 0], 'flatten_4': [4, 0], 'dense_8': [3, 0], 'dropout_29': [3, 0], 're_lu_24': [3, 0], 'dense_9': [3, 0], 'softmax_4': [7, 0]}\n",
      "fusing batch normalization to conv1d_16\n",
      "original weight max 0.16783513 min -0.16480944\n",
      "original bias max 1.2479011e-06 min -4.5471265e-07\n",
      "fused weight max 0.00019051899 min -0.00015557065\n",
      "fused bias max 0.09007082 min -0.089288026\n",
      "quantizing weights for layer conv1d_16\n",
      "    tensor_conv1d_16_kernel_0 dec bit 19\n",
      "    tensor_conv1d_16_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_16\n",
      "fusing batch normalization to conv1d_17\n",
      "original weight max 0.1799373 min -0.18349959\n",
      "original bias max 0.00017401401 min -0.00021557495\n",
      "fused weight max 0.2387793 min -0.2468963\n",
      "fused bias max 1.1586447 min -1.1047052\n",
      "quantizing weights for layer conv1d_17\n",
      "    tensor_conv1d_17_kernel_0 dec bit 9\n",
      "    tensor_conv1d_17_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_18\n",
      "original weight max 0.21860592 min -0.20380616\n",
      "original bias max 0.00036213713 min -0.00021738428\n",
      "fused weight max 0.34203973 min -0.32213452\n",
      "fused bias max 1.2673911 min -0.68055713\n",
      "quantizing weights for layer conv1d_18\n",
      "    tensor_conv1d_18_kernel_0 dec bit 8\n",
      "    tensor_conv1d_18_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_17\n",
      "quantizing weights for layer batch_normalization_18\n",
      "fusing batch normalization to conv1d_19\n",
      "original weight max 0.16601478 min -0.18116052\n",
      "original bias max 0.0002015102 min -0.0003503178\n",
      "fused weight max 0.14218457 min -0.12851073\n",
      "fused bias max 1.2382679 min -0.99492484\n",
      "quantizing weights for layer conv1d_19\n",
      "    tensor_conv1d_19_kernel_0 dec bit 9\n",
      "    tensor_conv1d_19_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_19\n",
      "quantizing weights for layer dense_8\n",
      "    tensor_dense_8_kernel_0 dec bit 9\n",
      "    tensor_dense_8_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_9\n",
      "    tensor_dense_9_kernel_0 dec bit 8\n",
      "    tensor_dense_9_bias_0 dec bit 11\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 1.8873 - accuracy: 0.3594 - val_loss: 1.7925 - val_accuracy: 0.4531 - 3s/epoch - 434ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.8982 - accuracy: 0.7812 - val_loss: 0.9794 - val_accuracy: 0.6406 - 144ms/epoch - 24ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.5185 - accuracy: 0.8828 - val_loss: 0.9247 - val_accuracy: 0.7500 - 138ms/epoch - 23ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3036 - accuracy: 0.9375 - val_loss: 0.9857 - val_accuracy: 0.7500 - 142ms/epoch - 24ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2134 - accuracy: 0.9648 - val_loss: 0.8501 - val_accuracy: 0.7969 - 128ms/epoch - 21ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1827 - accuracy: 0.9648 - val_loss: 0.7615 - val_accuracy: 0.8125 - 132ms/epoch - 22ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.0894 - accuracy: 0.9805 - val_loss: 0.5735 - val_accuracy: 0.8594 - 129ms/epoch - 22ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0782 - accuracy: 0.9805 - val_loss: 0.4631 - val_accuracy: 0.9062 - 145ms/epoch - 24ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0546 - accuracy: 0.9922 - val_loss: 0.3429 - val_accuracy: 0.9219 - 126ms/epoch - 21ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0668 - accuracy: 0.9922 - val_loss: 0.2729 - val_accuracy: 0.9062 - 124ms/epoch - 21ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0556 - accuracy: 0.9844 - val_loss: 0.2180 - val_accuracy: 0.9219 - 143ms/epoch - 24ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0375 - accuracy: 0.9922 - val_loss: 0.1747 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0355 - accuracy: 0.9961 - val_loss: 0.1503 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0412 - accuracy: 0.9922 - val_loss: 0.1563 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.1427 - val_accuracy: 0.9531 - 145ms/epoch - 24ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0306 - accuracy: 0.9883 - val_loss: 0.1150 - val_accuracy: 0.9531 - 137ms/epoch - 23ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9688 - 121ms/epoch - 20ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0160 - accuracy: 0.9961 - val_loss: 0.1096 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0923 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.0889 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0136 - accuracy: 0.9961 - val_loss: 0.1150 - val_accuracy: 0.9531 - 149ms/epoch - 25ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0331 - accuracy: 0.9922 - val_loss: 0.1548 - val_accuracy: 0.9531 - 143ms/epoch - 24ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.2709 - val_accuracy: 0.9531 - 145ms/epoch - 24ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0256 - accuracy: 0.9922 - val_loss: 0.2696 - val_accuracy: 0.9531 - 144ms/epoch - 24ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0232 - accuracy: 0.9922 - val_loss: 0.2152 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1687 - val_accuracy: 0.9531 - 163ms/epoch - 27ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.1562 - val_accuracy: 0.9531 - 150ms/epoch - 25ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0107 - accuracy: 0.9961 - val_loss: 0.1677 - val_accuracy: 0.9531 - 147ms/epoch - 25ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1795 - val_accuracy: 0.9531 - 157ms/epoch - 26ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.1837 - val_accuracy: 0.9531 - 132ms/epoch - 22ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1630 - val_accuracy: 0.9688 - 147ms/epoch - 25ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.1555 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0101 - accuracy: 0.9961 - val_loss: 0.1683 - val_accuracy: 0.9688 - 127ms/epoch - 21ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0234 - accuracy: 0.9961 - val_loss: 0.1535 - val_accuracy: 0.9531 - 126ms/epoch - 21ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0240 - accuracy: 0.9883 - val_loss: 0.1706 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0221 - accuracy: 0.9922 - val_loss: 0.2046 - val_accuracy: 0.9688 - 123ms/epoch - 21ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0192 - accuracy: 0.9922 - val_loss: 0.2120 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0180 - accuracy: 0.9922 - val_loss: 0.1758 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9688 - 116ms/epoch - 19ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1728 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1832 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1727 - val_accuracy: 0.9688 - 121ms/epoch - 20ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1622 - val_accuracy: 0.9688 - 126ms/epoch - 21ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1451 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9688 - 141ms/epoch - 23ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.1692 - val_accuracy: 0.9688 - 132ms/epoch - 22ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1904 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0202 - accuracy: 0.9922 - val_loss: 0.1991 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9688 - 117ms/epoch - 19ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0065 - accuracy: 0.9961 - val_loss: 0.1970 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0187 - accuracy: 0.9961 - val_loss: 0.2288 - val_accuracy: 0.9688 - 135ms/epoch - 22ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0257 - accuracy: 0.9961 - val_loss: 0.1826 - val_accuracy: 0.9688 - 132ms/epoch - 22ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0570 - accuracy: 0.9883 - val_loss: 0.1362 - val_accuracy: 0.9688 - 122ms/epoch - 20ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0256 - accuracy: 0.9883 - val_loss: 0.1113 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0123 - accuracy: 0.9922 - val_loss: 0.1249 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0943 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9844 - 114ms/epoch - 19ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9844 - 129ms/epoch - 21ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9844 - 135ms/epoch - 22ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9688 - 132ms/epoch - 22ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9688 - 122ms/epoch - 20ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9688 - 116ms/epoch - 19ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 0.9688 - 126ms/epoch - 21ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1041 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1177 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1680 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9688 - 113ms/epoch - 19ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2017 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 4.3917e-04 - accuracy: 1.0000 - val_loss: 0.2039 - val_accuracy: 0.9688 - 127ms/epoch - 21ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9688 - 139ms/epoch - 23ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 8.0209e-04 - accuracy: 1.0000 - val_loss: 0.2038 - val_accuracy: 0.9688 - 145ms/epoch - 24ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 7.1165e-04 - accuracy: 1.0000 - val_loss: 0.2020 - val_accuracy: 0.9688 - 147ms/epoch - 24ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 6.2056e-04 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9688 - 138ms/epoch - 23ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 7.8109e-04 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 7.9347e-04 - accuracy: 1.0000 - val_loss: 0.1988 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 0.9961 - val_loss: 0.1763 - val_accuracy: 0.9688 - 111ms/epoch - 19ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0404 - accuracy: 0.9805 - val_loss: 0.1058 - val_accuracy: 0.9531 - 126ms/epoch - 21ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0307 - accuracy: 0.9922 - val_loss: 0.0946 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.0297 - accuracy: 0.9883 - val_loss: 0.0998 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0141 - accuracy: 0.9922 - val_loss: 0.1347 - val_accuracy: 0.9688 - 136ms/epoch - 23ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.9531 - 112ms/epoch - 19ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0162 - accuracy: 0.9922 - val_loss: 0.1925 - val_accuracy: 0.9688 - 129ms/epoch - 22ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.0091 - accuracy: 0.9961 - val_loss: 0.2519 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0090 - accuracy: 0.9961 - val_loss: 0.2190 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1773 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9688 - 135ms/epoch - 23ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1380 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0081 - accuracy: 0.9961 - val_loss: 0.1372 - val_accuracy: 0.9688 - 173ms/epoch - 29ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 0.9961 - val_loss: 0.1333 - val_accuracy: 0.9688 - 123ms/epoch - 20ms/step\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1451 - accuracy: 0.9625\n",
      "input_6 Quantized method: max-min  Values max: 23883.0 min: -28326.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d_20 Quantized method: max-min  Values max: 25057.666 min: -16938.467 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_20 Quantized method: max-min  Values max: 9.530492 min: -8.0925455 dec bit 3\n",
      "dropout_30 Quantized method: max-min  Values max: 9.530492 min: -8.0925455 dec bit 3\n",
      "re_lu_25 Quantized method: max-min  Values max: 9.530492 min: -8.0925455 dec bit 3\n",
      "max_pooling1d_20 Quantized method: max-min  Values max: 9.530492 min: -8.0925455 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_21 Quantized method: max-min  Values max: 7.4203463 min: -6.3296194 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_22 Quantized method: max-min  Values max: 8.396901 min: -9.496376 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_21 Quantized method: max-min  Values max: 4.8769565 min: -5.9919143 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_22 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "dropout_31 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "dropout_32 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "re_lu_26 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "re_lu_27 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "max_pooling1d_23 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "max_pooling1d_21 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "max_pooling1d_22 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "dropout_33 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "concatenate_5 Quantized method: max-min  Values max: 6.176594 min: -8.643461 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_23 Quantized method: max-min  Values max: 9.767189 min: -8.032775 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "batch_normalization_23 Quantized method: max-min  Values max: 5.4261565 min: -3.9850042 dec bit 4\n",
      "re_lu_28 Quantized method: max-min  Values max: 5.4261565 min: -3.9850042 dec bit 4\n",
      "dropout_34 Quantized method: max-min  Values max: 5.4261565 min: -3.9850042 dec bit 4\n",
      "flatten_5 Quantized method: max-min  Values max: 5.4261565 min: -3.9850042 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_10 Quantized method: max-min  Values max: 9.700627 min: -8.751803 dec bit 3\n",
      "dropout_35 Quantized method: max-min  Values max: 9.700627 min: -8.751803 dec bit 3\n",
      "re_lu_29 Quantized method: max-min  Values max: 9.700627 min: -8.751803 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "dense_11 Quantized method: max-min  Values max: 17.238298 min: -10.382527 dec bit 2\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "softmax_5 Quantized method: max-min  Values max: 0.9999999 min: 1.5986509e-11 dec bit 7\n",
      "set dec bit 3 for the input of concatenate_5 : ['max_pooling1d_21', 'max_pooling1d_22', 'dropout_33']\n",
      "quantisation list {'input_6': [-8, 0], 'conv1d_20': [3, 0], 'batch_normalization_20': [3, 0], 'dropout_30': [3, 0], 're_lu_25': [3, 0], 'max_pooling1d_20': [3, 0], 'conv1d_21': [3, 0], 'conv1d_22': [3, 0], 'batch_normalization_21': [3, 0], 'batch_normalization_22': [3, 0], 'dropout_31': [3, 0], 'dropout_32': [3, 0], 're_lu_26': [3, 0], 're_lu_27': [3, 0], 'max_pooling1d_23': [3, 0], 'max_pooling1d_21': [3, 0], 'max_pooling1d_22': [3, 0], 'dropout_33': [3, 0], 'concatenate_5': [3, 0], 'conv1d_23': [4, 0], 'batch_normalization_23': [4, 0], 're_lu_28': [4, 0], 'dropout_34': [4, 0], 'flatten_5': [4, 0], 'dense_10': [3, 0], 'dropout_35': [3, 0], 're_lu_29': [3, 0], 'dense_11': [2, 0], 'softmax_5': [7, 0]}\n",
      "fusing batch normalization to conv1d_20\n",
      "original weight max 0.17419097 min -0.15479131\n",
      "original bias max 8.5348813e-07 min -4.0544785e-07\n",
      "fused weight max 0.0001967327 min -0.00016934318\n",
      "fused bias max 0.05089487 min -0.10468526\n",
      "quantizing weights for layer conv1d_20\n",
      "    tensor_conv1d_20_kernel_0 dec bit 19\n",
      "    tensor_conv1d_20_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_20\n",
      "fusing batch normalization to conv1d_21\n",
      "original weight max 0.19111215 min -0.17802179\n",
      "original bias max 0.0001881872 min -0.00045128993\n",
      "fused weight max 0.2498333 min -0.24335264\n",
      "fused bias max 1.1801821 min -1.1839628\n",
      "quantizing weights for layer conv1d_21\n",
      "    tensor_conv1d_21_kernel_0 dec bit 9\n",
      "    tensor_conv1d_21_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_22\n",
      "original weight max 0.21533422 min -0.2097326\n",
      "original bias max 0.00016964145 min -0.00036678364\n",
      "fused weight max 0.3734248 min -0.38535073\n",
      "fused bias max 1.1100684 min -0.96168023\n",
      "quantizing weights for layer conv1d_22\n",
      "    tensor_conv1d_22_kernel_0 dec bit 8\n",
      "    tensor_conv1d_22_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_21\n",
      "quantizing weights for layer batch_normalization_22\n",
      "fusing batch normalization to conv1d_23\n",
      "original weight max 0.16933061 min -0.16679893\n",
      "original bias max 0.00031373987 min -0.00023484853\n",
      "fused weight max 0.13530865 min -0.14567868\n",
      "fused bias max 0.98490226 min -0.79392767\n",
      "quantizing weights for layer conv1d_23\n",
      "    tensor_conv1d_23_kernel_0 dec bit 9\n",
      "    tensor_conv1d_23_bias_0 dec bit 7\n",
      "quantizing weights for layer batch_normalization_23\n",
      "quantizing weights for layer dense_10\n",
      "    tensor_dense_10_kernel_0 dec bit 9\n",
      "    tensor_dense_10_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_11\n",
      "    tensor_dense_11_kernel_0 dec bit 8\n",
      "    tensor_dense_11_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 1.9687 - accuracy: 0.3281 - val_loss: 2.0701 - val_accuracy: 0.4062 - 3s/epoch - 435ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.9032 - accuracy: 0.7266 - val_loss: 1.4649 - val_accuracy: 0.5469 - 163ms/epoch - 27ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.4925 - accuracy: 0.8828 - val_loss: 1.1209 - val_accuracy: 0.6875 - 134ms/epoch - 22ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3057 - accuracy: 0.9102 - val_loss: 0.8003 - val_accuracy: 0.8125 - 134ms/epoch - 22ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2175 - accuracy: 0.9414 - val_loss: 0.6341 - val_accuracy: 0.7969 - 128ms/epoch - 21ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1296 - accuracy: 0.9727 - val_loss: 0.4810 - val_accuracy: 0.8906 - 119ms/epoch - 20ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.0860 - accuracy: 0.9883 - val_loss: 0.3896 - val_accuracy: 0.9219 - 121ms/epoch - 20ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0933 - accuracy: 0.9727 - val_loss: 0.3554 - val_accuracy: 0.9219 - 137ms/epoch - 23ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0418 - accuracy: 0.9922 - val_loss: 0.3189 - val_accuracy: 0.9375 - 134ms/epoch - 22ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0455 - accuracy: 0.9805 - val_loss: 0.2893 - val_accuracy: 0.9375 - 123ms/epoch - 21ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0311 - accuracy: 0.9883 - val_loss: 0.2718 - val_accuracy: 0.9375 - 131ms/epoch - 22ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0367 - accuracy: 0.9883 - val_loss: 0.2522 - val_accuracy: 0.9531 - 148ms/epoch - 25ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0247 - accuracy: 0.9961 - val_loss: 0.2365 - val_accuracy: 0.9688 - 132ms/epoch - 22ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0276 - accuracy: 0.9961 - val_loss: 0.2753 - val_accuracy: 0.9844 - 128ms/epoch - 21ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0378 - accuracy: 0.9922 - val_loss: 0.2643 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0299 - accuracy: 0.9922 - val_loss: 0.2887 - val_accuracy: 0.9844 - 124ms/epoch - 21ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0366 - accuracy: 0.9922 - val_loss: 0.3038 - val_accuracy: 0.9375 - 133ms/epoch - 22ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0595 - accuracy: 0.9805 - val_loss: 0.2838 - val_accuracy: 0.9375 - 116ms/epoch - 19ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0282 - accuracy: 0.9922 - val_loss: 0.2885 - val_accuracy: 0.9375 - 138ms/epoch - 23ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0425 - accuracy: 0.9844 - val_loss: 0.2570 - val_accuracy: 0.9531 - 131ms/epoch - 22ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0418 - accuracy: 0.9922 - val_loss: 0.2544 - val_accuracy: 0.9531 - 140ms/epoch - 23ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9531 - 128ms/epoch - 21ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0176 - accuracy: 0.9961 - val_loss: 0.2589 - val_accuracy: 0.9531 - 116ms/epoch - 19ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0185 - accuracy: 0.9961 - val_loss: 0.2366 - val_accuracy: 0.9531 - 126ms/epoch - 21ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.1921 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0389 - accuracy: 0.9883 - val_loss: 0.1998 - val_accuracy: 0.9688 - 133ms/epoch - 22ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0204 - accuracy: 0.9961 - val_loss: 0.2137 - val_accuracy: 0.9688 - 116ms/epoch - 19ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0186 - accuracy: 0.9961 - val_loss: 0.2410 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 0.9531 - 170ms/epoch - 28ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0180 - accuracy: 0.9961 - val_loss: 0.2170 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.2161 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2210 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0096 - accuracy: 0.9961 - val_loss: 0.2222 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.2224 - val_accuracy: 0.9688 - 129ms/epoch - 22ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 0.9961 - val_loss: 0.2198 - val_accuracy: 0.9688 - 121ms/epoch - 20ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0095 - accuracy: 0.9961 - val_loss: 0.2172 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2179 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 0.9961 - val_loss: 0.2270 - val_accuracy: 0.9688 - 123ms/epoch - 21ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2314 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9688 - 127ms/epoch - 21ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2295 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2431 - val_accuracy: 0.9531 - 135ms/epoch - 22ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2550 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9531 - 129ms/epoch - 22ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2668 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0174 - accuracy: 0.9922 - val_loss: 0.2427 - val_accuracy: 0.9844 - 115ms/epoch - 19ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0202 - accuracy: 0.9922 - val_loss: 0.2445 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9531 - 142ms/epoch - 24ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2321 - val_accuracy: 0.9531 - 115ms/epoch - 19ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 0.9688 - 132ms/epoch - 22ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2271 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2310 - val_accuracy: 0.9688 - 116ms/epoch - 19ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 9.2595e-04 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9531 - 120ms/epoch - 20ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2408 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2374 - val_accuracy: 0.9531 - 123ms/epoch - 21ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2137 - val_accuracy: 0.9688 - 149ms/epoch - 25ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.9688 - 143ms/epoch - 24ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0146 - accuracy: 0.9961 - val_loss: 0.2231 - val_accuracy: 0.9531 - 118ms/epoch - 20ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0183 - accuracy: 0.9922 - val_loss: 0.3507 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0144 - accuracy: 0.9961 - val_loss: 0.3481 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0278 - accuracy: 0.9922 - val_loss: 0.2932 - val_accuracy: 0.9531 - 116ms/epoch - 19ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0248 - accuracy: 0.9883 - val_loss: 0.2852 - val_accuracy: 0.9531 - 116ms/epoch - 19ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0195 - accuracy: 0.9922 - val_loss: 0.2741 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0142 - accuracy: 0.9961 - val_loss: 0.3737 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0096 - accuracy: 0.9961 - val_loss: 0.4491 - val_accuracy: 0.9531 - 114ms/epoch - 19ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0403 - accuracy: 0.9883 - val_loss: 0.3824 - val_accuracy: 0.9531 - 140ms/epoch - 23ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.2954 - val_accuracy: 0.9531 - 131ms/epoch - 22ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.2545 - val_accuracy: 0.9531 - 145ms/epoch - 24ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9688 - 113ms/epoch - 19ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0201 - accuracy: 0.9922 - val_loss: 0.2470 - val_accuracy: 0.9688 - 140ms/epoch - 23ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 0.9688 - 136ms/epoch - 23ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9531 - 151ms/epoch - 25ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0078 - accuracy: 0.9961 - val_loss: 0.2578 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 7.4907e-04 - accuracy: 1.0000 - val_loss: 0.2837 - val_accuracy: 0.9531 - 146ms/epoch - 24ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.0085 - accuracy: 0.9961 - val_loss: 0.3092 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3106 - val_accuracy: 0.9531 - 145ms/epoch - 24ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 8.7516e-04 - accuracy: 1.0000 - val_loss: 0.3104 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3086 - val_accuracy: 0.9531 - 128ms/epoch - 21ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3068 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 8.1252e-04 - accuracy: 1.0000 - val_loss: 0.3054 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 9.6758e-04 - accuracy: 1.0000 - val_loss: 0.3017 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9531 - 160ms/epoch - 27ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2948 - val_accuracy: 0.9531 - 144ms/epoch - 24ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2900 - val_accuracy: 0.9531 - 135ms/epoch - 23ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 6.2541e-04 - accuracy: 1.0000 - val_loss: 0.2874 - val_accuracy: 0.9531 - 170ms/epoch - 28ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 6.5034e-04 - accuracy: 1.0000 - val_loss: 0.2870 - val_accuracy: 0.9531 - 140ms/epoch - 23ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2927 - val_accuracy: 0.9531 - 130ms/epoch - 22ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 0.9961 - val_loss: 0.2908 - val_accuracy: 0.9531 - 157ms/epoch - 26ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2794 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.2868 - val_accuracy: 0.9531 - 113ms/epoch - 19ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0120 - accuracy: 0.9922 - val_loss: 0.2956 - val_accuracy: 0.9531 - 126ms/epoch - 21ms/step\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "input_7 Quantized method: max-min  Values max: 24034.0 min: -20908.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d_24 Quantized method: max-min  Values max: 16787.541 min: -13712.889 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "batch_normalization_24 Quantized method: max-min  Values max: 6.061866 min: -7.9306073 dec bit 4\n",
      "dropout_36 Quantized method: max-min  Values max: 6.061866 min: -7.9306073 dec bit 4\n",
      "re_lu_30 Quantized method: max-min  Values max: 6.061866 min: -7.9306073 dec bit 4\n",
      "max_pooling1d_24 Quantized method: max-min  Values max: 6.061866 min: -7.9306073 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_25 Quantized method: max-min  Values max: 5.7939186 min: -7.443083 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_26 Quantized method: max-min  Values max: 6.3025303 min: -6.086885 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_25 Quantized method: max-min  Values max: 5.7493825 min: -5.87253 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_26 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "dropout_37 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "dropout_38 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "re_lu_31 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "re_lu_32 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "max_pooling1d_27 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "max_pooling1d_25 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "max_pooling1d_26 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "dropout_39 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "concatenate_6 Quantized method: max-min  Values max: 5.4755416 min: -4.8177047 dec bit 4\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "conv1d_27 Quantized method: max-min  Values max: 9.136018 min: -8.260722 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "batch_normalization_27 Quantized method: max-min  Values max: 4.942647 min: -3.8629193 dec bit 4\n",
      "re_lu_33 Quantized method: max-min  Values max: 4.942647 min: -3.8629193 dec bit 4\n",
      "dropout_40 Quantized method: max-min  Values max: 4.942647 min: -3.8629193 dec bit 4\n",
      "flatten_6 Quantized method: max-min  Values max: 4.942647 min: -3.8629193 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_12 Quantized method: max-min  Values max: 9.52188 min: -10.031869 dec bit 3\n",
      "dropout_41 Quantized method: max-min  Values max: 9.52188 min: -10.031869 dec bit 3\n",
      "re_lu_34 Quantized method: max-min  Values max: 9.52188 min: -10.031869 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "dense_13 Quantized method: max-min  Values max: 19.295275 min: -12.541446 dec bit 2\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "softmax_6 Quantized method: max-min  Values max: 1.0 min: 4.3518642e-14 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_6 : ['max_pooling1d_25', 'max_pooling1d_26', 'dropout_39']\n",
      "quantisation list {'input_7': [-8, 0], 'conv1d_24': [4, 0], 'batch_normalization_24': [4, 0], 'dropout_36': [4, 0], 're_lu_30': [4, 0], 'max_pooling1d_24': [4, 0], 'conv1d_25': [4, 0], 'conv1d_26': [4, 0], 'batch_normalization_25': [4, 0], 'batch_normalization_26': [4, 0], 'dropout_37': [4, 0], 'dropout_38': [4, 0], 're_lu_31': [4, 0], 're_lu_32': [4, 0], 'max_pooling1d_27': [4, 0], 'max_pooling1d_25': [4, 0], 'max_pooling1d_26': [4, 0], 'dropout_39': [4, 0], 'concatenate_6': [4, 0], 'conv1d_27': [4, 0], 'batch_normalization_27': [4, 0], 're_lu_33': [4, 0], 'dropout_40': [4, 0], 'flatten_6': [4, 0], 'dense_12': [3, 0], 'dropout_41': [3, 0], 're_lu_34': [3, 0], 'dense_13': [2, 0], 'softmax_6': [7, 0]}\n",
      "fusing batch normalization to conv1d_24\n",
      "original weight max 0.16406745 min -0.15883565\n",
      "original bias max 4.3559274e-07 min -4.7317027e-07\n",
      "fused weight max 0.00017992622 min -0.00017116964\n",
      "fused bias max 0.07370672 min -0.116828\n",
      "quantizing weights for layer conv1d_24\n",
      "    tensor_conv1d_24_kernel_0 dec bit 19\n",
      "    tensor_conv1d_24_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_24\n",
      "fusing batch normalization to conv1d_25\n",
      "original weight max 0.18333761 min -0.17306177\n",
      "original bias max 0.00017831492 min -0.0001764698\n",
      "fused weight max 0.25531498 min -0.21791945\n",
      "fused bias max 1.1671958 min -0.7993733\n",
      "quantizing weights for layer conv1d_25\n",
      "    tensor_conv1d_25_kernel_0 dec bit 8\n",
      "    tensor_conv1d_25_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_26\n",
      "original weight max 0.22869745 min -0.22322145\n",
      "original bias max 0.0002744773 min -0.00033237052\n",
      "fused weight max 0.34737745 min -0.3427763\n",
      "fused bias max 1.0051751 min -0.9986007\n",
      "quantizing weights for layer conv1d_26\n",
      "    tensor_conv1d_26_kernel_0 dec bit 8\n",
      "    tensor_conv1d_26_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_25\n",
      "quantizing weights for layer batch_normalization_26\n",
      "fusing batch normalization to conv1d_27\n",
      "original weight max 0.16976689 min -0.16146146\n",
      "original bias max 0.00018077191 min -0.00032748116\n",
      "fused weight max 0.12291394 min -0.12123102\n",
      "fused bias max 1.0286404 min -1.0921273\n",
      "quantizing weights for layer conv1d_27\n",
      "    tensor_conv1d_27_kernel_0 dec bit 10\n",
      "    tensor_conv1d_27_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_27\n",
      "quantizing weights for layer dense_12\n",
      "    tensor_dense_12_kernel_0 dec bit 9\n",
      "    tensor_dense_12_bias_0 dec bit 13\n",
      "quantizing weights for layer dense_13\n",
      "    tensor_dense_13_kernel_0 dec bit 8\n",
      "    tensor_dense_13_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 2.0403 - accuracy: 0.3203 - val_loss: 1.8875 - val_accuracy: 0.4375 - 3s/epoch - 497ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.9165 - accuracy: 0.7773 - val_loss: 1.1826 - val_accuracy: 0.5781 - 148ms/epoch - 25ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.4829 - accuracy: 0.8867 - val_loss: 0.8252 - val_accuracy: 0.7500 - 151ms/epoch - 25ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3071 - accuracy: 0.9375 - val_loss: 0.5753 - val_accuracy: 0.8125 - 163ms/epoch - 27ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2211 - accuracy: 0.9492 - val_loss: 0.4363 - val_accuracy: 0.8750 - 160ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1151 - accuracy: 0.9805 - val_loss: 0.3865 - val_accuracy: 0.8750 - 145ms/epoch - 24ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1245 - accuracy: 0.9727 - val_loss: 0.4286 - val_accuracy: 0.8594 - 125ms/epoch - 21ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0822 - accuracy: 0.9766 - val_loss: 0.3708 - val_accuracy: 0.8906 - 135ms/epoch - 23ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0584 - accuracy: 0.9961 - val_loss: 0.3003 - val_accuracy: 0.9062 - 120ms/epoch - 20ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0769 - accuracy: 0.9805 - val_loss: 0.2950 - val_accuracy: 0.9219 - 126ms/epoch - 21ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0515 - accuracy: 0.9844 - val_loss: 0.4863 - val_accuracy: 0.8594 - 124ms/epoch - 21ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0917 - accuracy: 0.9727 - val_loss: 0.3573 - val_accuracy: 0.8750 - 150ms/epoch - 25ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0399 - accuracy: 0.9961 - val_loss: 0.2716 - val_accuracy: 0.8750 - 123ms/epoch - 21ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0420 - accuracy: 0.9922 - val_loss: 0.2173 - val_accuracy: 0.8750 - 123ms/epoch - 21ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0309 - accuracy: 0.9961 - val_loss: 0.1553 - val_accuracy: 0.9219 - 145ms/epoch - 24ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 0.9375 - 157ms/epoch - 26ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0228 - accuracy: 0.9961 - val_loss: 0.1225 - val_accuracy: 0.9219 - 142ms/epoch - 24ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0302 - accuracy: 0.9922 - val_loss: 0.1704 - val_accuracy: 0.9219 - 151ms/epoch - 25ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0381 - accuracy: 0.9961 - val_loss: 0.1459 - val_accuracy: 0.9062 - 146ms/epoch - 24ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0269 - accuracy: 0.9961 - val_loss: 0.1188 - val_accuracy: 0.9375 - 182ms/epoch - 30ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0181 - accuracy: 0.9961 - val_loss: 0.0780 - val_accuracy: 0.9844 - 150ms/epoch - 25ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9844 - 150ms/epoch - 25ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9688 - 158ms/epoch - 26ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0636 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0166 - accuracy: 0.9961 - val_loss: 0.0604 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0180 - accuracy: 0.9922 - val_loss: 0.0549 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0509 - accuracy: 0.9805 - val_loss: 0.1548 - val_accuracy: 0.9375 - 125ms/epoch - 21ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0987 - accuracy: 0.9609 - val_loss: 0.0617 - val_accuracy: 0.9844 - 119ms/epoch - 20ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9688 - 138ms/epoch - 23ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0287 - accuracy: 0.9883 - val_loss: 0.0758 - val_accuracy: 0.9688 - 137ms/epoch - 23ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0176 - accuracy: 0.9961 - val_loss: 0.0371 - val_accuracy: 0.9844 - 152ms/epoch - 25ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000 - 131ms/epoch - 22ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0155 - accuracy: 0.9961 - val_loss: 0.0295 - val_accuracy: 1.0000 - 173ms/epoch - 29ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0284 - val_accuracy: 1.0000 - 144ms/epoch - 24ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 0.9844 - 113ms/epoch - 19ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0402 - val_accuracy: 0.9844 - 130ms/epoch - 22ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9844 - 136ms/epoch - 23ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9844 - 116ms/epoch - 19ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9688 - 137ms/epoch - 23ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 0.9688 - 132ms/epoch - 22ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9688 - 113ms/epoch - 19ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9688 - 129ms/epoch - 21ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0082 - accuracy: 0.9961 - val_loss: 0.0831 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 0.9688 - 115ms/epoch - 19ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 0.9961 - val_loss: 0.0746 - val_accuracy: 0.9688 - 152ms/epoch - 25ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9688 - 176ms/epoch - 29ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9688 - 139ms/epoch - 23ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9688 - 113ms/epoch - 19ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9688 - 159ms/epoch - 27ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 8.6150e-04 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0749 - val_accuracy: 0.9688 - 123ms/epoch - 20ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9688 - 113ms/epoch - 19ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0609 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0665 - val_accuracy: 0.9688 - 144ms/epoch - 24ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 9.0443e-04 - accuracy: 1.0000 - val_loss: 0.0688 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9688 - 133ms/epoch - 22ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0710 - val_accuracy: 0.9688 - 135ms/epoch - 23ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9688 - 140ms/epoch - 23ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9688 - 136ms/epoch - 23ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0649 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0691 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0719 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0746 - val_accuracy: 0.9688 - 123ms/epoch - 20ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0891 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 8.4969e-04 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9688 - 137ms/epoch - 23ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9688 - 144ms/epoch - 24ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1040 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9688 - 122ms/epoch - 20ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9688 - 130ms/epoch - 22ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 7.3766e-04 - accuracy: 1.0000 - val_loss: 0.0908 - val_accuracy: 0.9688 - 118ms/epoch - 20ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 7.4117e-04 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 0.9688 - 127ms/epoch - 21ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 8.9010e-04 - accuracy: 1.0000 - val_loss: 0.0885 - val_accuracy: 0.9688 - 123ms/epoch - 21ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 7.0176e-04 - accuracy: 1.0000 - val_loss: 0.0885 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0877 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0884 - val_accuracy: 0.9688 - 151ms/epoch - 25ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0955 - val_accuracy: 0.9688 - 124ms/epoch - 21ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9688 - 138ms/epoch - 23ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 5.1692e-04 - accuracy: 1.0000 - val_loss: 0.0903 - val_accuracy: 0.9688 - 120ms/epoch - 20ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 9.7258e-04 - accuracy: 1.0000 - val_loss: 0.0886 - val_accuracy: 0.9688 - 144ms/epoch - 24ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0877 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0894 - val_accuracy: 0.9688 - 155ms/epoch - 26ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 7.3535e-04 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1188 - val_accuracy: 0.9688 - 129ms/epoch - 21ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9688 - 143ms/epoch - 24ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1170 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9688 - 153ms/epoch - 26ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9688 - 167ms/epoch - 28ms/step\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0659 - accuracy: 0.9875\n",
      "input_8 Quantized method: max-min  Values max: 25168.0 min: -23248.0 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_28 Quantized method: max-min  Values max: 13791.712 min: -20691.986 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_28 Quantized method: max-min  Values max: 6.1538744 min: -7.0592766 dec bit 4\n",
      "dropout_42 Quantized method: max-min  Values max: 6.1538744 min: -7.0592766 dec bit 4\n",
      "re_lu_35 Quantized method: max-min  Values max: 6.1538744 min: -7.0592766 dec bit 4\n",
      "max_pooling1d_28 Quantized method: max-min  Values max: 6.1538744 min: -7.0592766 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_29 Quantized method: max-min  Values max: 6.541837 min: -7.0970106 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_30 Quantized method: max-min  Values max: 5.672287 min: -9.735521 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_29 Quantized method: max-min  Values max: 4.938437 min: -5.3040395 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_30 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "dropout_43 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "dropout_44 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "re_lu_36 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "re_lu_37 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "max_pooling1d_31 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "max_pooling1d_29 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "max_pooling1d_30 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "dropout_45 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "concatenate_7 Quantized method: max-min  Values max: 5.528163 min: -4.98873 dec bit 4\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "conv1d_31 Quantized method: max-min  Values max: 8.517087 min: -7.0809407 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "batch_normalization_31 Quantized method: max-min  Values max: 4.257871 min: -3.552418 dec bit 4\n",
      "re_lu_38 Quantized method: max-min  Values max: 4.257871 min: -3.552418 dec bit 4\n",
      "dropout_46 Quantized method: max-min  Values max: 4.257871 min: -3.552418 dec bit 4\n",
      "flatten_7 Quantized method: max-min  Values max: 4.257871 min: -3.552418 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_14 Quantized method: max-min  Values max: 10.1785345 min: -6.170037 dec bit 3\n",
      "dropout_47 Quantized method: max-min  Values max: 10.1785345 min: -6.170037 dec bit 3\n",
      "re_lu_39 Quantized method: max-min  Values max: 10.1785345 min: -6.170037 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_15 Quantized method: max-min  Values max: 18.646057 min: -11.187024 dec bit 2\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "softmax_7 Quantized method: max-min  Values max: 0.99999964 min: 2.484367e-12 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_7 : ['max_pooling1d_29', 'max_pooling1d_30', 'dropout_45']\n",
      "quantisation list {'input_8': [-8, 0], 'conv1d_28': [4, 0], 'batch_normalization_28': [4, 0], 'dropout_42': [4, 0], 're_lu_35': [4, 0], 'max_pooling1d_28': [4, 0], 'conv1d_29': [4, 0], 'conv1d_30': [4, 0], 'batch_normalization_29': [4, 0], 'batch_normalization_30': [4, 0], 'dropout_43': [4, 0], 'dropout_44': [4, 0], 're_lu_36': [4, 0], 're_lu_37': [4, 0], 'max_pooling1d_31': [4, 0], 'max_pooling1d_29': [4, 0], 'max_pooling1d_30': [4, 0], 'dropout_45': [4, 0], 'concatenate_7': [4, 0], 'conv1d_31': [4, 0], 'batch_normalization_31': [4, 0], 're_lu_38': [4, 0], 'dropout_46': [4, 0], 'flatten_7': [4, 0], 'dense_14': [3, 0], 'dropout_47': [3, 0], 're_lu_39': [3, 0], 'dense_15': [2, 0], 'softmax_7': [7, 0]}\n",
      "fusing batch normalization to conv1d_28\n",
      "original weight max 0.15051635 min -0.15862907\n",
      "original bias max 3.3918843e-07 min -1.1945889e-07\n",
      "fused weight max 0.00021120773 min -0.00020007128\n",
      "fused bias max 0.07829397 min -0.09110611\n",
      "quantizing weights for layer conv1d_28\n",
      "    tensor_conv1d_28_kernel_0 dec bit 19\n",
      "    tensor_conv1d_28_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_28\n",
      "fusing batch normalization to conv1d_29\n",
      "original weight max 0.1735947 min -0.16399635\n",
      "original bias max 0.00010154329 min -0.00023419445\n",
      "fused weight max 0.25254408 min -0.23657292\n",
      "fused bias max 1.0471857 min -0.90246826\n",
      "quantizing weights for layer conv1d_29\n",
      "    tensor_conv1d_29_kernel_0 dec bit 8\n",
      "    tensor_conv1d_29_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_30\n",
      "original weight max 0.20632042 min -0.2042227\n",
      "original bias max 9.287688e-05 min -0.00010367275\n",
      "fused weight max 0.40498942 min -0.3956191\n",
      "fused bias max 1.0922033 min -0.84986615\n",
      "quantizing weights for layer conv1d_30\n",
      "    tensor_conv1d_30_kernel_0 dec bit 8\n",
      "    tensor_conv1d_30_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_29\n",
      "quantizing weights for layer batch_normalization_30\n",
      "fusing batch normalization to conv1d_31\n",
      "original weight max 0.16170686 min -0.16129848\n",
      "original bias max 0.00012207373 min -0.0001292099\n",
      "fused weight max 0.14243829 min -0.14570662\n",
      "fused bias max 1.2645967 min -1.0753871\n",
      "quantizing weights for layer conv1d_31\n",
      "    tensor_conv1d_31_kernel_0 dec bit 9\n",
      "    tensor_conv1d_31_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_31\n",
      "quantizing weights for layer dense_14\n",
      "    tensor_dense_14_kernel_0 dec bit 9\n",
      "    tensor_dense_14_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_15\n",
      "    tensor_dense_15_kernel_0 dec bit 8\n",
      "    tensor_dense_15_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 2.0694 - accuracy: 0.3477 - val_loss: 3.2903 - val_accuracy: 0.3438 - 3s/epoch - 432ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.9640 - accuracy: 0.7305 - val_loss: 1.8106 - val_accuracy: 0.5312 - 138ms/epoch - 23ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.5791 - accuracy: 0.8594 - val_loss: 1.0555 - val_accuracy: 0.5781 - 134ms/epoch - 22ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3733 - accuracy: 0.9023 - val_loss: 0.7851 - val_accuracy: 0.7344 - 126ms/epoch - 21ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2448 - accuracy: 0.9297 - val_loss: 0.5230 - val_accuracy: 0.9062 - 127ms/epoch - 21ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1597 - accuracy: 0.9492 - val_loss: 0.3705 - val_accuracy: 0.9062 - 146ms/epoch - 24ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1164 - accuracy: 0.9648 - val_loss: 0.3083 - val_accuracy: 0.9062 - 127ms/epoch - 21ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0924 - accuracy: 0.9766 - val_loss: 0.2051 - val_accuracy: 0.9062 - 147ms/epoch - 25ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0799 - accuracy: 0.9727 - val_loss: 0.1139 - val_accuracy: 0.9844 - 123ms/epoch - 20ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0555 - accuracy: 0.9844 - val_loss: 0.0990 - val_accuracy: 0.9688 - 126ms/epoch - 21ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0513 - accuracy: 0.9922 - val_loss: 0.1242 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0828 - accuracy: 0.9766 - val_loss: 0.1812 - val_accuracy: 0.9531 - 120ms/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0485 - accuracy: 0.9922 - val_loss: 0.1603 - val_accuracy: 0.9375 - 124ms/epoch - 21ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0380 - accuracy: 0.9922 - val_loss: 0.0995 - val_accuracy: 0.9688 - 143ms/epoch - 24ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0307 - accuracy: 0.9922 - val_loss: 0.0358 - val_accuracy: 1.0000 - 121ms/epoch - 20ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0283 - accuracy: 0.9922 - val_loss: 0.0180 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0182 - accuracy: 0.9961 - val_loss: 0.0142 - val_accuracy: 1.0000 - 131ms/epoch - 22ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0176 - accuracy: 0.9961 - val_loss: 0.0125 - val_accuracy: 1.0000 - 117ms/epoch - 19ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0243 - accuracy: 0.9961 - val_loss: 0.0106 - val_accuracy: 1.0000 - 143ms/epoch - 24ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0231 - accuracy: 0.9922 - val_loss: 0.0089 - val_accuracy: 1.0000 - 134ms/epoch - 22ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0075 - val_accuracy: 1.0000 - 131ms/epoch - 22ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0162 - accuracy: 0.9922 - val_loss: 0.0065 - val_accuracy: 1.0000 - 143ms/epoch - 24ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0251 - accuracy: 0.9922 - val_loss: 0.0051 - val_accuracy: 1.0000 - 124ms/epoch - 21ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0168 - accuracy: 0.9961 - val_loss: 0.0052 - val_accuracy: 1.0000 - 119ms/epoch - 20ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000 - 120ms/epoch - 20ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0240 - accuracy: 0.9922 - val_loss: 0.0042 - val_accuracy: 1.0000 - 138ms/epoch - 23ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0154 - accuracy: 0.9961 - val_loss: 0.0042 - val_accuracy: 1.0000 - 121ms/epoch - 20ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000 - 132ms/epoch - 22ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0169 - accuracy: 0.9961 - val_loss: 0.0037 - val_accuracy: 1.0000 - 170ms/epoch - 28ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0179 - accuracy: 0.9922 - val_loss: 0.0040 - val_accuracy: 1.0000 - 125ms/epoch - 21ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000 - 123ms/epoch - 20ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000 - 122ms/epoch - 20ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000 - 117ms/epoch - 19ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000 - 131ms/epoch - 22ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000 - 135ms/epoch - 23ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000 - 141ms/epoch - 24ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000 - 152ms/epoch - 25ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000 - 132ms/epoch - 22ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0192 - accuracy: 0.9961 - val_loss: 0.0014 - val_accuracy: 1.0000 - 116ms/epoch - 19ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000 - 130ms/epoch - 22ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000 - 145ms/epoch - 24ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0061 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 1.0000 - 144ms/epoch - 24ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 133ms/epoch - 22ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 155ms/epoch - 26ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 170ms/epoch - 28ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000 - 130ms/epoch - 22ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000 - 124ms/epoch - 21ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000 - 125ms/epoch - 21ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000 - 130ms/epoch - 22ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000 - 131ms/epoch - 22ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 128ms/epoch - 21ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 129ms/epoch - 22ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000 - 134ms/epoch - 22ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000 - 148ms/epoch - 25ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000 - 140ms/epoch - 23ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000 - 140ms/epoch - 23ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000 - 144ms/epoch - 24ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000 - 136ms/epoch - 23ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - 126ms/epoch - 21ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 8.5581e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - 155ms/epoch - 26ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - 145ms/epoch - 24ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000 - 144ms/epoch - 24ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - 153ms/epoch - 26ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 7.3575e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000 - 167ms/epoch - 28ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000 - 187ms/epoch - 31ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000 - 141ms/epoch - 24ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - 138ms/epoch - 23ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000 - 131ms/epoch - 22ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000 - 133ms/epoch - 22ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0079 - accuracy: 0.9961 - val_loss: 0.0028 - val_accuracy: 1.0000 - 140ms/epoch - 23ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000 - 142ms/epoch - 24ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0088 - accuracy: 0.9961 - val_loss: 0.0016 - val_accuracy: 1.0000 - 167ms/epoch - 28ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000 - 143ms/epoch - 24ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0174 - accuracy: 0.9961 - val_loss: 0.0033 - val_accuracy: 1.0000 - 137ms/epoch - 23ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0546 - accuracy: 0.9883 - val_loss: 0.2102 - val_accuracy: 0.9375 - 174ms/epoch - 29ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.0303 - val_accuracy: 0.9844 - 157ms/epoch - 26ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0359 - accuracy: 0.9883 - val_loss: 0.0060 - val_accuracy: 1.0000 - 150ms/epoch - 25ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0317 - accuracy: 0.9883 - val_loss: 0.0035 - val_accuracy: 1.0000 - 156ms/epoch - 26ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0168 - accuracy: 0.9961 - val_loss: 0.0044 - val_accuracy: 1.0000 - 127ms/epoch - 21ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0273 - accuracy: 0.9922 - val_loss: 0.0027 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0171 - accuracy: 0.9961 - val_loss: 0.0012 - val_accuracy: 1.0000 - 157ms/epoch - 26ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000 - 137ms/epoch - 23ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.0109 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 1.0000 - 136ms/epoch - 23ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - 129ms/epoch - 21ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0067 - accuracy: 0.9961 - val_loss: 0.0015 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 6.8600e-04 - val_accuracy: 1.0000 - 140ms/epoch - 23ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0106 - accuracy: 0.9961 - val_loss: 6.4174e-04 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 0.9961 - val_loss: 6.3500e-04 - val_accuracy: 1.0000 - 118ms/epoch - 20ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 7.9182e-04 - val_accuracy: 1.0000 - 127ms/epoch - 21ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000 - 150ms/epoch - 25ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000 - 132ms/epoch - 22ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0058 - accuracy: 0.9961 - val_loss: 0.0140 - val_accuracy: 0.9844 - 126ms/epoch - 21ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000 - 129ms/epoch - 22ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 7.4325e-04 - val_accuracy: 1.0000 - 130ms/epoch - 22ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 7.7856e-04 - val_accuracy: 1.0000 - 158ms/epoch - 26ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 6.7708e-04 - val_accuracy: 1.0000 - 158ms/epoch - 26ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0051 - accuracy: 0.9961 - val_loss: 5.0605e-04 - val_accuracy: 1.0000 - 135ms/epoch - 22ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7025e-04 - val_accuracy: 1.0000 - 130ms/epoch - 22ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7236e-04 - val_accuracy: 1.0000 - 135ms/epoch - 22ms/step\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3720 - accuracy: 0.9500\n",
      "input_9 Quantized method: max-min  Values max: 23883.0 min: -23109.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d_32 Quantized method: max-min  Values max: 16448.78 min: -12718.529 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_32 Quantized method: max-min  Values max: 5.3241386 min: -6.3001914 dec bit 4\n",
      "dropout_48 Quantized method: max-min  Values max: 5.3241386 min: -6.3001914 dec bit 4\n",
      "re_lu_40 Quantized method: max-min  Values max: 5.3241386 min: -6.3001914 dec bit 4\n",
      "max_pooling1d_32 Quantized method: max-min  Values max: 5.3241386 min: -6.3001914 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_33 Quantized method: max-min  Values max: 7.427774 min: -5.5786924 dec bit 4\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "conv1d_34 Quantized method: max-min  Values max: 6.4402194 min: -8.84733 dec bit 3\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_33 Quantized method: max-min  Values max: 5.360552 min: -5.2710633 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_34 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "dropout_49 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "dropout_50 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "re_lu_41 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "re_lu_42 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "max_pooling1d_35 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "max_pooling1d_33 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "max_pooling1d_34 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "dropout_51 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "concatenate_8 Quantized method: max-min  Values max: 5.3796186 min: -4.883338 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_35 Quantized method: max-min  Values max: 10.615769 min: -7.926271 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_35 Quantized method: max-min  Values max: 4.990158 min: -3.6708279 dec bit 4\n",
      "re_lu_43 Quantized method: max-min  Values max: 4.990158 min: -3.6708279 dec bit 4\n",
      "dropout_52 Quantized method: max-min  Values max: 4.990158 min: -3.6708279 dec bit 4\n",
      "flatten_8 Quantized method: max-min  Values max: 4.990158 min: -3.6708279 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_16 Quantized method: max-min  Values max: 12.584699 min: -13.883146 dec bit 3\n",
      "dropout_53 Quantized method: max-min  Values max: 12.584699 min: -13.883146 dec bit 3\n",
      "re_lu_44 Quantized method: max-min  Values max: 12.584699 min: -13.883146 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_17 Quantized method: max-min  Values max: 18.811928 min: -13.206611 dec bit 2\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "softmax_8 Quantized method: max-min  Values max: 1.0 min: 2.663547e-13 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_8 : ['max_pooling1d_33', 'max_pooling1d_34', 'dropout_51']\n",
      "quantisation list {'input_9': [-8, 0], 'conv1d_32': [4, 0], 'batch_normalization_32': [4, 0], 'dropout_48': [4, 0], 're_lu_40': [4, 0], 'max_pooling1d_32': [4, 0], 'conv1d_33': [4, 0], 'conv1d_34': [4, 0], 'batch_normalization_33': [4, 0], 'batch_normalization_34': [4, 0], 'dropout_49': [4, 0], 'dropout_50': [4, 0], 're_lu_41': [4, 0], 're_lu_42': [4, 0], 'max_pooling1d_35': [4, 0], 'max_pooling1d_33': [4, 0], 'max_pooling1d_34': [4, 0], 'dropout_51': [4, 0], 'concatenate_8': [4, 0], 'conv1d_35': [4, 0], 'batch_normalization_35': [4, 0], 're_lu_43': [4, 0], 'dropout_52': [4, 0], 'flatten_8': [4, 0], 'dense_16': [3, 0], 'dropout_53': [3, 0], 're_lu_44': [3, 0], 'dense_17': [2, 0], 'softmax_8': [7, 0]}\n",
      "fusing batch normalization to conv1d_32\n",
      "original weight max 0.17279041 min -0.15779406\n",
      "original bias max 3.0353982e-07 min -4.13533e-07\n",
      "fused weight max 0.0002490541 min -0.00021903618\n",
      "fused bias max 0.10648438 min -0.09567181\n",
      "quantizing weights for layer conv1d_32\n",
      "    tensor_conv1d_32_kernel_0 dec bit 18\n",
      "    tensor_conv1d_32_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_32\n",
      "fusing batch normalization to conv1d_33\n",
      "original weight max 0.17632431 min -0.1748001\n",
      "original bias max 0.00018446698 min -0.00024380615\n",
      "fused weight max 0.22882159 min -0.25916517\n",
      "fused bias max 1.0011208 min -0.9708423\n",
      "quantizing weights for layer conv1d_33\n",
      "    tensor_conv1d_33_kernel_0 dec bit 8\n",
      "    tensor_conv1d_33_bias_0 dec bit 7\n",
      "fusing batch normalization to conv1d_34\n",
      "original weight max 0.20522533 min -0.20807348\n",
      "original bias max 0.00018930295 min -0.00031797003\n",
      "fused weight max 0.3774035 min -0.36025083\n",
      "fused bias max 1.0354036 min -0.9198921\n",
      "quantizing weights for layer conv1d_34\n",
      "    tensor_conv1d_34_kernel_0 dec bit 8\n",
      "    tensor_conv1d_34_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_33\n",
      "quantizing weights for layer batch_normalization_34\n",
      "fusing batch normalization to conv1d_35\n",
      "original weight max 0.16133393 min -0.1710832\n",
      "original bias max 0.00021661205 min -0.0002450256\n",
      "fused weight max 0.12548368 min -0.12354334\n",
      "fused bias max 1.3623363 min -0.83778095\n",
      "quantizing weights for layer conv1d_35\n",
      "    tensor_conv1d_35_kernel_0 dec bit 10\n",
      "    tensor_conv1d_35_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_35\n",
      "quantizing weights for layer dense_16\n",
      "    tensor_dense_16_kernel_0 dec bit 9\n",
      "    tensor_dense_16_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_17\n",
      "    tensor_dense_17_kernel_0 dec bit 8\n",
      "    tensor_dense_17_bias_0 dec bit 11\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 1.9416 - accuracy: 0.2773 - val_loss: 1.5941 - val_accuracy: 0.4688 - 3s/epoch - 453ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.9110 - accuracy: 0.7812 - val_loss: 0.9742 - val_accuracy: 0.7812 - 150ms/epoch - 25ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.4946 - accuracy: 0.8906 - val_loss: 1.0962 - val_accuracy: 0.8750 - 187ms/epoch - 31ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.2878 - accuracy: 0.9375 - val_loss: 1.0617 - val_accuracy: 0.8750 - 148ms/epoch - 25ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.1858 - accuracy: 0.9688 - val_loss: 1.0837 - val_accuracy: 0.8750 - 162ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1234 - accuracy: 0.9727 - val_loss: 0.9946 - val_accuracy: 0.8594 - 157ms/epoch - 26ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1351 - accuracy: 0.9688 - val_loss: 0.7573 - val_accuracy: 0.8750 - 144ms/epoch - 24ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0727 - accuracy: 0.9844 - val_loss: 0.6515 - val_accuracy: 0.8906 - 150ms/epoch - 25ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0457 - accuracy: 0.9922 - val_loss: 0.6166 - val_accuracy: 0.9062 - 141ms/epoch - 24ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0556 - accuracy: 0.9844 - val_loss: 0.6609 - val_accuracy: 0.9062 - 149ms/epoch - 25ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0616 - accuracy: 0.9883 - val_loss: 0.6095 - val_accuracy: 0.9062 - 141ms/epoch - 23ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0439 - accuracy: 0.9883 - val_loss: 0.5371 - val_accuracy: 0.9062 - 139ms/epoch - 23ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0325 - accuracy: 0.9922 - val_loss: 0.5041 - val_accuracy: 0.9219 - 130ms/epoch - 22ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0300 - accuracy: 0.9883 - val_loss: 0.4694 - val_accuracy: 0.9219 - 136ms/epoch - 23ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.4201 - val_accuracy: 0.9219 - 127ms/epoch - 21ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0207 - accuracy: 0.9922 - val_loss: 0.3871 - val_accuracy: 0.9219 - 127ms/epoch - 21ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.9219 - 171ms/epoch - 29ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.3182 - val_accuracy: 0.9219 - 174ms/epoch - 29ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.2349 - val_accuracy: 0.9375 - 141ms/epoch - 23ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0197 - accuracy: 0.9961 - val_loss: 0.2138 - val_accuracy: 0.9375 - 134ms/epoch - 22ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0193 - accuracy: 0.9961 - val_loss: 0.2445 - val_accuracy: 0.9375 - 129ms/epoch - 21ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.9375 - 152ms/epoch - 25ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0188 - accuracy: 0.9961 - val_loss: 0.2907 - val_accuracy: 0.9375 - 128ms/epoch - 21ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9375 - 129ms/epoch - 21ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2663 - val_accuracy: 0.9375 - 147ms/epoch - 24ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2572 - val_accuracy: 0.9375 - 133ms/epoch - 22ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 0.9375 - 131ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 0.9375 - 139ms/epoch - 23ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2262 - val_accuracy: 0.9375 - 119ms/epoch - 20ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2281 - val_accuracy: 0.9375 - 122ms/epoch - 20ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0109 - accuracy: 0.9961 - val_loss: 0.2087 - val_accuracy: 0.9375 - 126ms/epoch - 21ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.2071 - val_accuracy: 0.9375 - 121ms/epoch - 20ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.2024 - val_accuracy: 0.9375 - 131ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9375 - 155ms/epoch - 26ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0086 - accuracy: 0.9961 - val_loss: 0.1927 - val_accuracy: 0.9375 - 133ms/epoch - 22ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9688 - 134ms/epoch - 22ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0190 - accuracy: 0.9922 - val_loss: 0.2278 - val_accuracy: 0.9375 - 116ms/epoch - 19ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5206 - val_accuracy: 0.9062 - 119ms/epoch - 20ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0344 - accuracy: 0.9922 - val_loss: 0.4166 - val_accuracy: 0.9219 - 136ms/epoch - 23ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0287 - accuracy: 0.9961 - val_loss: 0.1842 - val_accuracy: 0.9375 - 121ms/epoch - 20ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0614 - accuracy: 0.9844 - val_loss: 0.1325 - val_accuracy: 0.9531 - 147ms/epoch - 24ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0409 - accuracy: 0.9844 - val_loss: 0.1762 - val_accuracy: 0.9219 - 134ms/epoch - 22ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9219 - 123ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0167 - accuracy: 0.9961 - val_loss: 0.2011 - val_accuracy: 0.9219 - 132ms/epoch - 22ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.1403 - val_accuracy: 0.9219 - 120ms/epoch - 20ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0822 - val_accuracy: 0.9531 - 113ms/epoch - 19ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0253 - accuracy: 0.9883 - val_loss: 0.1321 - val_accuracy: 0.9219 - 149ms/epoch - 25ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9219 - 133ms/epoch - 22ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0135 - accuracy: 0.9961 - val_loss: 0.1518 - val_accuracy: 0.9219 - 144ms/epoch - 24ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9531 - 123ms/epoch - 21ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9531 - 115ms/epoch - 19ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0158 - accuracy: 0.9961 - val_loss: 0.1238 - val_accuracy: 0.9375 - 118ms/epoch - 20ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1414 - val_accuracy: 0.9375 - 126ms/epoch - 21ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1474 - val_accuracy: 0.9375 - 113ms/epoch - 19ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9375 - 123ms/epoch - 20ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 0.9961 - val_loss: 0.1409 - val_accuracy: 0.9375 - 128ms/epoch - 21ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.9375 - 116ms/epoch - 19ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0878 - val_accuracy: 0.9375 - 127ms/epoch - 21ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9531 - 115ms/epoch - 19ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 0.9531 - 124ms/epoch - 21ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0091 - accuracy: 0.9961 - val_loss: 0.0987 - val_accuracy: 0.9531 - 127ms/epoch - 21ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9531 - 111ms/epoch - 19ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1235 - val_accuracy: 0.9531 - 123ms/epoch - 20ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9531 - 117ms/epoch - 20ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9531 - 120ms/epoch - 20ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9531 - 147ms/epoch - 24ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1256 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9531 - 115ms/epoch - 19ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9531 - 117ms/epoch - 19ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 9.3205e-04 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9375 - 115ms/epoch - 19ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 9.5720e-04 - accuracy: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.9375 - 120ms/epoch - 20ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9375 - 112ms/epoch - 19ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9375 - 119ms/epoch - 20ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 8.3684e-04 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9375 - 120ms/epoch - 20ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 8.5130e-04 - accuracy: 1.0000 - val_loss: 0.1038 - val_accuracy: 0.9375 - 122ms/epoch - 20ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 5.2668e-04 - accuracy: 1.0000 - val_loss: 0.1023 - val_accuracy: 0.9531 - 120ms/epoch - 20ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 6.3217e-04 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 0.9531 - 136ms/epoch - 23ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9531 - 120ms/epoch - 20ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9375 - 121ms/epoch - 20ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 7.5864e-04 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 0.9375 - 110ms/epoch - 18ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 9.4614e-04 - accuracy: 1.0000 - val_loss: 0.1012 - val_accuracy: 0.9375 - 120ms/epoch - 20ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 6.5764e-04 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9375 - 118ms/epoch - 20ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 6.4530e-04 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9375 - 120ms/epoch - 20ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0100 - accuracy: 0.9961 - val_loss: 0.0790 - val_accuracy: 0.9688 - 151ms/epoch - 25ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0200 - accuracy: 0.9961 - val_loss: 0.1226 - val_accuracy: 0.9531 - 121ms/epoch - 20ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0371 - accuracy: 0.9844 - val_loss: 0.1369 - val_accuracy: 0.9688 - 114ms/epoch - 19ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 0.0151 - accuracy: 0.9922 - val_loss: 0.1818 - val_accuracy: 0.9531 - 116ms/epoch - 19ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9531 - 116ms/epoch - 19ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9375 - 112ms/epoch - 19ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9375 - 113ms/epoch - 19ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9375 - 136ms/epoch - 23ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1719 - val_accuracy: 0.9375 - 155ms/epoch - 26ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1676 - val_accuracy: 0.9375 - 148ms/epoch - 25ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 0.9375 - 131ms/epoch - 22ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9375 - 138ms/epoch - 23ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1905 - val_accuracy: 0.9375 - 123ms/epoch - 20ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1919 - val_accuracy: 0.9375 - 112ms/epoch - 19ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9375 - 136ms/epoch - 23ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1754 - val_accuracy: 0.9375 - 120ms/epoch - 20ms/step\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "input_10 Quantized method: max-min  Values max: 23339.0 min: -23248.0 dec bit -8\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "conv1d_36 Quantized method: max-min  Values max: 14193.127 min: -15284.52 dec bit -7\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "batch_normalization_36 Quantized method: max-min  Values max: 6.2216883 min: -5.882553 dec bit 4\n",
      "dropout_54 Quantized method: max-min  Values max: 6.2216883 min: -5.882553 dec bit 4\n",
      "re_lu_45 Quantized method: max-min  Values max: 6.2216883 min: -5.882553 dec bit 4\n",
      "max_pooling1d_36 Quantized method: max-min  Values max: 6.2216883 min: -5.882553 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_37 Quantized method: max-min  Values max: 5.3504395 min: -6.3165917 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_38 Quantized method: max-min  Values max: 6.3978243 min: -7.38055 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_37 Quantized method: max-min  Values max: 4.539263 min: -4.823778 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_38 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "dropout_55 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "dropout_56 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "re_lu_46 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "re_lu_47 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "max_pooling1d_39 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "max_pooling1d_37 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "max_pooling1d_38 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "dropout_57 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "concatenate_9 Quantized method: max-min  Values max: 5.1480303 min: -5.3325505 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_39 Quantized method: max-min  Values max: 9.734339 min: -8.045743 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_39 Quantized method: max-min  Values max: 4.8135815 min: -3.581046 dec bit 4\n",
      "re_lu_48 Quantized method: max-min  Values max: 4.8135815 min: -3.581046 dec bit 4\n",
      "dropout_58 Quantized method: max-min  Values max: 4.8135815 min: -3.581046 dec bit 4\n",
      "flatten_9 Quantized method: max-min  Values max: 4.8135815 min: -3.581046 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_18 Quantized method: max-min  Values max: 10.46411 min: -8.572305 dec bit 3\n",
      "dropout_59 Quantized method: max-min  Values max: 10.46411 min: -8.572305 dec bit 3\n",
      "re_lu_49 Quantized method: max-min  Values max: 10.46411 min: -8.572305 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_19 Quantized method: max-min  Values max: 16.896957 min: -9.658526 dec bit 2\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "softmax_9 Quantized method: max-min  Values max: 0.9999995 min: 2.9315378e-12 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_9 : ['max_pooling1d_37', 'max_pooling1d_38', 'dropout_57']\n",
      "quantisation list {'input_10': [-8, 0], 'conv1d_36': [4, 0], 'batch_normalization_36': [4, 0], 'dropout_54': [4, 0], 're_lu_45': [4, 0], 'max_pooling1d_36': [4, 0], 'conv1d_37': [4, 0], 'conv1d_38': [4, 0], 'batch_normalization_37': [4, 0], 'batch_normalization_38': [4, 0], 'dropout_55': [4, 0], 'dropout_56': [4, 0], 're_lu_46': [4, 0], 're_lu_47': [4, 0], 'max_pooling1d_39': [4, 0], 'max_pooling1d_37': [4, 0], 'max_pooling1d_38': [4, 0], 'dropout_57': [4, 0], 'concatenate_9': [4, 0], 'conv1d_39': [4, 0], 'batch_normalization_39': [4, 0], 're_lu_48': [4, 0], 'dropout_58': [4, 0], 'flatten_9': [4, 0], 'dense_18': [3, 0], 'dropout_59': [3, 0], 're_lu_49': [3, 0], 'dense_19': [2, 0], 'softmax_9': [7, 0]}\n",
      "fusing batch normalization to conv1d_36\n",
      "original weight max 0.18368201 min -0.1725314\n",
      "original bias max 4.8553414e-07 min -3.4017427e-07\n",
      "fused weight max 0.00015271138 min -0.0001496611\n",
      "fused bias max 0.06306541 min -0.0913949\n",
      "quantizing weights for layer conv1d_36\n",
      "    tensor_conv1d_36_kernel_0 dec bit 19\n",
      "    tensor_conv1d_36_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_36\n",
      "fusing batch normalization to conv1d_37\n",
      "original weight max 0.1836685 min -0.17755301\n",
      "original bias max 0.0002113909 min -0.00036217045\n",
      "fused weight max 0.26332465 min -0.20778282\n",
      "fused bias max 0.9320183 min -0.7957731\n",
      "quantizing weights for layer conv1d_37\n",
      "    tensor_conv1d_37_kernel_0 dec bit 8\n",
      "    tensor_conv1d_37_bias_0 dec bit 7\n",
      "fusing batch normalization to conv1d_38\n",
      "original weight max 0.21929133 min -0.222814\n",
      "original bias max 0.0003458962 min -0.00027273752\n",
      "fused weight max 0.3425892 min -0.299094\n",
      "fused bias max 1.038397 min -0.9721057\n",
      "quantizing weights for layer conv1d_38\n",
      "    tensor_conv1d_38_kernel_0 dec bit 8\n",
      "    tensor_conv1d_38_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_37\n",
      "quantizing weights for layer batch_normalization_38\n",
      "fusing batch normalization to conv1d_39\n",
      "original weight max 0.18268156 min -0.16453682\n",
      "original bias max 0.000116212665 min -0.0002393055\n",
      "fused weight max 0.1488803 min -0.13453089\n",
      "fused bias max 1.3018243 min -0.9861781\n",
      "quantizing weights for layer conv1d_39\n",
      "    tensor_conv1d_39_kernel_0 dec bit 9\n",
      "    tensor_conv1d_39_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_39\n",
      "quantizing weights for layer dense_18\n",
      "    tensor_dense_18_kernel_0 dec bit 9\n",
      "    tensor_dense_18_bias_0 dec bit 11\n",
      "quantizing weights for layer dense_19\n",
      "    tensor_dense_19_kernel_0 dec bit 8\n",
      "    tensor_dense_19_bias_0 dec bit 12\n",
      "Epoch 1/100\n",
      "6/6 - 3s - loss: 1.7874 - accuracy: 0.4023 - val_loss: 1.6965 - val_accuracy: 0.5000 - 3s/epoch - 445ms/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.7710 - accuracy: 0.7578 - val_loss: 1.6963 - val_accuracy: 0.5781 - 148ms/epoch - 25ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.4810 - accuracy: 0.8750 - val_loss: 1.4089 - val_accuracy: 0.7656 - 149ms/epoch - 25ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 0s - loss: 0.3170 - accuracy: 0.9453 - val_loss: 1.1791 - val_accuracy: 0.8125 - 142ms/epoch - 24ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.2343 - accuracy: 0.9570 - val_loss: 1.1116 - val_accuracy: 0.7969 - 142ms/epoch - 24ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 0s - loss: 0.1694 - accuracy: 0.9648 - val_loss: 1.0630 - val_accuracy: 0.8281 - 147ms/epoch - 24ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.1018 - accuracy: 0.9805 - val_loss: 0.9687 - val_accuracy: 0.8125 - 133ms/epoch - 22ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 0s - loss: 0.0656 - accuracy: 0.9805 - val_loss: 0.7744 - val_accuracy: 0.8750 - 130ms/epoch - 22ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0624 - accuracy: 0.9805 - val_loss: 0.6279 - val_accuracy: 0.8906 - 129ms/epoch - 22ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.0532 - accuracy: 0.9883 - val_loss: 0.5532 - val_accuracy: 0.8906 - 141ms/epoch - 23ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0298 - accuracy: 0.9922 - val_loss: 0.4992 - val_accuracy: 0.9062 - 131ms/epoch - 22ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.8750 - 121ms/epoch - 20ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0232 - accuracy: 0.9961 - val_loss: 0.4566 - val_accuracy: 0.8438 - 132ms/epoch - 22ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 0s - loss: 0.0278 - accuracy: 0.9961 - val_loss: 0.4808 - val_accuracy: 0.8438 - 146ms/epoch - 24ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.4988 - val_accuracy: 0.8594 - 171ms/epoch - 28ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0327 - accuracy: 0.9961 - val_loss: 0.4610 - val_accuracy: 0.8906 - 149ms/epoch - 25ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0294 - accuracy: 0.9922 - val_loss: 0.4021 - val_accuracy: 0.9219 - 143ms/epoch - 24ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0183 - accuracy: 0.9961 - val_loss: 0.3898 - val_accuracy: 0.9219 - 136ms/epoch - 23ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 0.9062 - 146ms/epoch - 24ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.4263 - val_accuracy: 0.9062 - 119ms/epoch - 20ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 0s - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.4401 - val_accuracy: 0.9062 - 110ms/epoch - 18ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 0s - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.4643 - val_accuracy: 0.9062 - 140ms/epoch - 23ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4776 - val_accuracy: 0.9062 - 184ms/epoch - 31ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.9062 - 189ms/epoch - 31ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0076 - accuracy: 0.9961 - val_loss: 0.4528 - val_accuracy: 0.9062 - 154ms/epoch - 26ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.4193 - val_accuracy: 0.9062 - 154ms/epoch - 26ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0069 - accuracy: 0.9961 - val_loss: 0.4014 - val_accuracy: 0.9062 - 136ms/epoch - 23ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 0.9062 - 134ms/epoch - 22ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.9062 - 146ms/epoch - 24ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.4243 - val_accuracy: 0.9062 - 148ms/epoch - 25ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4244 - val_accuracy: 0.9062 - 193ms/epoch - 32ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0065 - accuracy: 0.9961 - val_loss: 0.4221 - val_accuracy: 0.9062 - 155ms/epoch - 26ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 0.8906 - 137ms/epoch - 23ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0171 - accuracy: 0.9961 - val_loss: 0.4477 - val_accuracy: 0.9062 - 146ms/epoch - 24ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.9062 - 127ms/epoch - 21ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4148 - val_accuracy: 0.9062 - 118ms/epoch - 20ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 0.9062 - 145ms/epoch - 24ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0172 - accuracy: 0.9961 - val_loss: 0.3857 - val_accuracy: 0.8906 - 142ms/epoch - 24ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3736 - val_accuracy: 0.9062 - 150ms/epoch - 25ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0079 - accuracy: 0.9961 - val_loss: 0.3657 - val_accuracy: 0.9062 - 119ms/epoch - 20ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9062 - 137ms/epoch - 23ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.3361 - val_accuracy: 0.9062 - 152ms/epoch - 25ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.3333 - val_accuracy: 0.9062 - 117ms/epoch - 19ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3403 - val_accuracy: 0.9062 - 150ms/epoch - 25ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3495 - val_accuracy: 0.9062 - 147ms/epoch - 25ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3626 - val_accuracy: 0.9062 - 129ms/epoch - 22ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.3809 - val_accuracy: 0.9062 - 172ms/epoch - 29ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9062 - 125ms/epoch - 21ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4054 - val_accuracy: 0.9062 - 141ms/epoch - 24ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4101 - val_accuracy: 0.9062 - 125ms/epoch - 21ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 0.9961 - val_loss: 0.4200 - val_accuracy: 0.9062 - 128ms/epoch - 21ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 0.9062 - 126ms/epoch - 21ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.4422 - val_accuracy: 0.9062 - 118ms/epoch - 20ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4551 - val_accuracy: 0.9062 - 108ms/epoch - 18ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9062 - 133ms/epoch - 22ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 9.3134e-04 - accuracy: 1.0000 - val_loss: 0.4238 - val_accuracy: 0.8906 - 122ms/epoch - 20ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.4126 - val_accuracy: 0.8906 - 129ms/epoch - 22ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4006 - val_accuracy: 0.8906 - 128ms/epoch - 21ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0055 - accuracy: 0.9961 - val_loss: 0.3793 - val_accuracy: 0.8906 - 127ms/epoch - 21ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9062 - 134ms/epoch - 22ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.3443 - val_accuracy: 0.9062 - 151ms/epoch - 25ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4053 - val_accuracy: 0.9062 - 128ms/epoch - 21ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.3994 - val_accuracy: 0.9062 - 120ms/epoch - 20ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0207 - accuracy: 0.9922 - val_loss: 0.3007 - val_accuracy: 0.9375 - 134ms/epoch - 22ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2947 - val_accuracy: 0.9062 - 127ms/epoch - 21ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0156 - accuracy: 0.9961 - val_loss: 0.4491 - val_accuracy: 0.8906 - 114ms/epoch - 19ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.9062 - 138ms/epoch - 23ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.8906 - 149ms/epoch - 25ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0103 - accuracy: 0.9961 - val_loss: 0.5336 - val_accuracy: 0.8750 - 119ms/epoch - 20ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4967 - val_accuracy: 0.8906 - 154ms/epoch - 26ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4804 - val_accuracy: 0.8906 - 116ms/epoch - 19ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.4813 - val_accuracy: 0.9062 - 118ms/epoch - 20ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4803 - val_accuracy: 0.9219 - 142ms/epoch - 24ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.9062 - 164ms/epoch - 27ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.9062 - 139ms/epoch - 23ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5121 - val_accuracy: 0.9062 - 159ms/epoch - 27ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.4934 - val_accuracy: 0.9062 - 141ms/epoch - 23ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4795 - val_accuracy: 0.9062 - 134ms/epoch - 22ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4741 - val_accuracy: 0.9062 - 142ms/epoch - 24ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4751 - val_accuracy: 0.9062 - 132ms/epoch - 22ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 4.7121e-04 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.9062 - 127ms/epoch - 21ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 8.4472e-04 - accuracy: 1.0000 - val_loss: 0.4821 - val_accuracy: 0.9062 - 122ms/epoch - 20ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 6.2219e-04 - accuracy: 1.0000 - val_loss: 0.4848 - val_accuracy: 0.9062 - 151ms/epoch - 25ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 7.4256e-04 - accuracy: 1.0000 - val_loss: 0.4885 - val_accuracy: 0.9062 - 144ms/epoch - 24ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4905 - val_accuracy: 0.9062 - 160ms/epoch - 27ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4941 - val_accuracy: 0.9062 - 116ms/epoch - 19ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4902 - val_accuracy: 0.9062 - 130ms/epoch - 22ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 9.2543e-04 - accuracy: 1.0000 - val_loss: 0.4867 - val_accuracy: 0.9062 - 150ms/epoch - 25ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 9.7843e-04 - accuracy: 1.0000 - val_loss: 0.4843 - val_accuracy: 0.9062 - 189ms/epoch - 31ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 6.7457e-04 - accuracy: 1.0000 - val_loss: 0.4825 - val_accuracy: 0.9062 - 142ms/epoch - 24ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 7.3314e-04 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 0.9062 - 135ms/epoch - 23ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 3.9425e-04 - accuracy: 1.0000 - val_loss: 0.4749 - val_accuracy: 0.9062 - 168ms/epoch - 28ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 3.6628e-04 - accuracy: 1.0000 - val_loss: 0.4746 - val_accuracy: 0.9062 - 146ms/epoch - 24ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 6.8517e-04 - accuracy: 1.0000 - val_loss: 0.4761 - val_accuracy: 0.9062 - 173ms/epoch - 29ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 5.9181e-04 - accuracy: 1.0000 - val_loss: 0.4770 - val_accuracy: 0.9062 - 145ms/epoch - 24ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 5.7046e-04 - accuracy: 1.0000 - val_loss: 0.4846 - val_accuracy: 0.9062 - 208ms/epoch - 35ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4915 - val_accuracy: 0.9062 - 305ms/epoch - 51ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 6.2918e-04 - accuracy: 1.0000 - val_loss: 0.4806 - val_accuracy: 0.9062 - 223ms/epoch - 37ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 2.7352e-04 - accuracy: 1.0000 - val_loss: 0.4739 - val_accuracy: 0.9062 - 180ms/epoch - 30ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 4.8772e-04 - accuracy: 1.0000 - val_loss: 0.4753 - val_accuracy: 0.9062 - 203ms/epoch - 34ms/step\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "input_11 Quantized method: max-min  Values max: 25168.0 min: -23730.0 dec bit -8\n",
      "3/3 [==============================] - 2s 3ms/step\n",
      "conv1d_40 Quantized method: max-min  Values max: 21042.43 min: -17794.967 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_40 Quantized method: max-min  Values max: 6.2578263 min: -6.787385 dec bit 4\n",
      "dropout_60 Quantized method: max-min  Values max: 6.2578263 min: -6.787385 dec bit 4\n",
      "re_lu_50 Quantized method: max-min  Values max: 6.2578263 min: -6.787385 dec bit 4\n",
      "max_pooling1d_40 Quantized method: max-min  Values max: 6.2578263 min: -6.787385 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_41 Quantized method: max-min  Values max: 6.9089913 min: -5.5130134 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_42 Quantized method: max-min  Values max: 7.807512 min: -4.757816 dec bit 4\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_41 Quantized method: max-min  Values max: 5.1425014 min: -5.0741496 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_42 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "dropout_61 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "dropout_62 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "re_lu_51 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "re_lu_52 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "max_pooling1d_43 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "max_pooling1d_41 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "max_pooling1d_42 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "dropout_63 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "concatenate_10 Quantized method: max-min  Values max: 5.770377 min: -5.240596 dec bit 4\n",
      "3/3 [==============================] - 1s 4ms/step\n",
      "conv1d_43 Quantized method: max-min  Values max: 10.55955 min: -7.3712974 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_43 Quantized method: max-min  Values max: 4.657234 min: -4.0509605 dec bit 4\n",
      "re_lu_53 Quantized method: max-min  Values max: 4.657234 min: -4.0509605 dec bit 4\n",
      "dropout_64 Quantized method: max-min  Values max: 4.657234 min: -4.0509605 dec bit 4\n",
      "flatten_10 Quantized method: max-min  Values max: 4.657234 min: -4.0509605 dec bit 4\n",
      "3/3 [==============================] - 1s 5ms/step\n",
      "dense_20 Quantized method: max-min  Values max: 9.434252 min: -7.8799133 dec bit 3\n",
      "dropout_65 Quantized method: max-min  Values max: 9.434252 min: -7.8799133 dec bit 3\n",
      "re_lu_54 Quantized method: max-min  Values max: 9.434252 min: -7.8799133 dec bit 3\n",
      "3/3 [==============================] - 1s 9ms/step\n",
      "dense_21 Quantized method: max-min  Values max: 16.534035 min: -10.095995 dec bit 2\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "softmax_10 Quantized method: max-min  Values max: 0.9999995 min: 3.056665e-12 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_10 : ['max_pooling1d_41', 'max_pooling1d_42', 'dropout_63']\n",
      "quantisation list {'input_11': [-8, 0], 'conv1d_40': [4, 0], 'batch_normalization_40': [4, 0], 'dropout_60': [4, 0], 're_lu_50': [4, 0], 'max_pooling1d_40': [4, 0], 'conv1d_41': [4, 0], 'conv1d_42': [4, 0], 'batch_normalization_41': [4, 0], 'batch_normalization_42': [4, 0], 'dropout_61': [4, 0], 'dropout_62': [4, 0], 're_lu_51': [4, 0], 're_lu_52': [4, 0], 'max_pooling1d_43': [4, 0], 'max_pooling1d_41': [4, 0], 'max_pooling1d_42': [4, 0], 'dropout_63': [4, 0], 'concatenate_10': [4, 0], 'conv1d_43': [4, 0], 'batch_normalization_43': [4, 0], 're_lu_53': [4, 0], 'dropout_64': [4, 0], 'flatten_10': [4, 0], 'dense_20': [3, 0], 'dropout_65': [3, 0], 're_lu_54': [3, 0], 'dense_21': [2, 0], 'softmax_10': [7, 0]}\n",
      "fusing batch normalization to conv1d_40\n",
      "original weight max 0.16390944 min -0.16512956\n",
      "original bias max 1.7257243e-07 min -2.4904284e-07\n",
      "fused weight max 0.0002409699 min -0.00020271412\n",
      "fused bias max 0.0778347 min -0.08539121\n",
      "quantizing weights for layer conv1d_40\n",
      "    tensor_conv1d_40_kernel_0 dec bit 19\n",
      "    tensor_conv1d_40_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_40\n",
      "fusing batch normalization to conv1d_41\n",
      "original weight max 0.16570294 min -0.16783367\n",
      "original bias max 0.0002150205 min -0.00011468796\n",
      "fused weight max 0.25692174 min -0.25503927\n",
      "fused bias max 1.0396383 min -0.99192363\n",
      "quantizing weights for layer conv1d_41\n",
      "    tensor_conv1d_41_kernel_0 dec bit 8\n",
      "    tensor_conv1d_41_bias_0 dec bit 6\n",
      "fusing batch normalization to conv1d_42\n",
      "original weight max 0.2014465 min -0.19731127\n",
      "original bias max 0.00011050081 min -0.00011438221\n",
      "fused weight max 0.34257364 min -0.33827263\n",
      "fused bias max 1.1315626 min -1.1098309\n",
      "quantizing weights for layer conv1d_42\n",
      "    tensor_conv1d_42_kernel_0 dec bit 8\n",
      "    tensor_conv1d_42_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_41\n",
      "quantizing weights for layer batch_normalization_42\n",
      "fusing batch normalization to conv1d_43\n",
      "original weight max 0.15804832 min -0.1515138\n",
      "original bias max 9.084939e-05 min -0.00013680782\n",
      "fused weight max 0.13475618 min -0.14049178\n",
      "fused bias max 1.0061591 min -1.005756\n",
      "quantizing weights for layer conv1d_43\n",
      "    tensor_conv1d_43_kernel_0 dec bit 9\n",
      "    tensor_conv1d_43_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_43\n",
      "quantizing weights for layer dense_20\n",
      "    tensor_dense_20_kernel_0 dec bit 9\n",
      "    tensor_dense_20_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_21\n",
      "    tensor_dense_21_kernel_0 dec bit 8\n",
      "    tensor_dense_21_bias_0 dec bit 13\n",
      "Epoch 1/100\n",
      "6/6 - 17s - loss: 1.8455 - accuracy: 0.3828 - val_loss: 2.0252 - val_accuracy: 0.5469 - 17s/epoch - 3s/step\n",
      "Epoch 2/100\n",
      "6/6 - 0s - loss: 0.6977 - accuracy: 0.8398 - val_loss: 1.2551 - val_accuracy: 0.6406 - 171ms/epoch - 28ms/step\n",
      "Epoch 3/100\n",
      "6/6 - 0s - loss: 0.3926 - accuracy: 0.9062 - val_loss: 1.3823 - val_accuracy: 0.7656 - 162ms/epoch - 27ms/step\n",
      "Epoch 4/100\n",
      "6/6 - 1s - loss: 0.2838 - accuracy: 0.9258 - val_loss: 1.1785 - val_accuracy: 0.7656 - 576ms/epoch - 96ms/step\n",
      "Epoch 5/100\n",
      "6/6 - 0s - loss: 0.1778 - accuracy: 0.9570 - val_loss: 0.8577 - val_accuracy: 0.8281 - 163ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "6/6 - 5s - loss: 0.1393 - accuracy: 0.9570 - val_loss: 0.6150 - val_accuracy: 0.8750 - 5s/epoch - 794ms/step\n",
      "Epoch 7/100\n",
      "6/6 - 0s - loss: 0.0776 - accuracy: 0.9805 - val_loss: 0.5439 - val_accuracy: 0.8750 - 164ms/epoch - 27ms/step\n",
      "Epoch 8/100\n",
      "6/6 - 1s - loss: 0.0928 - accuracy: 0.9766 - val_loss: 0.4514 - val_accuracy: 0.9062 - 797ms/epoch - 133ms/step\n",
      "Epoch 9/100\n",
      "6/6 - 0s - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.5186 - val_accuracy: 0.9062 - 178ms/epoch - 30ms/step\n",
      "Epoch 10/100\n",
      "6/6 - 0s - loss: 0.1572 - accuracy: 0.9570 - val_loss: 0.4678 - val_accuracy: 0.9062 - 153ms/epoch - 25ms/step\n",
      "Epoch 11/100\n",
      "6/6 - 0s - loss: 0.0600 - accuracy: 0.9883 - val_loss: 0.3892 - val_accuracy: 0.9688 - 168ms/epoch - 28ms/step\n",
      "Epoch 12/100\n",
      "6/6 - 0s - loss: 0.0352 - accuracy: 0.9961 - val_loss: 0.3986 - val_accuracy: 0.9688 - 157ms/epoch - 26ms/step\n",
      "Epoch 13/100\n",
      "6/6 - 0s - loss: 0.0528 - accuracy: 0.9844 - val_loss: 0.3968 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 14/100\n",
      "6/6 - 2s - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9688 - 2s/epoch - 294ms/step\n",
      "Epoch 15/100\n",
      "6/6 - 0s - loss: 0.0272 - accuracy: 0.9961 - val_loss: 0.3387 - val_accuracy: 0.9688 - 149ms/epoch - 25ms/step\n",
      "Epoch 16/100\n",
      "6/6 - 0s - loss: 0.0216 - accuracy: 0.9961 - val_loss: 0.3167 - val_accuracy: 0.9688 - 151ms/epoch - 25ms/step\n",
      "Epoch 17/100\n",
      "6/6 - 0s - loss: 0.0331 - accuracy: 0.9883 - val_loss: 0.3106 - val_accuracy: 0.9688 - 158ms/epoch - 26ms/step\n",
      "Epoch 18/100\n",
      "6/6 - 0s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.3036 - val_accuracy: 0.9688 - 155ms/epoch - 26ms/step\n",
      "Epoch 19/100\n",
      "6/6 - 0s - loss: 0.0193 - accuracy: 0.9961 - val_loss: 0.2998 - val_accuracy: 0.9688 - 160ms/epoch - 27ms/step\n",
      "Epoch 20/100\n",
      "6/6 - 0s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.9688 - 152ms/epoch - 25ms/step\n",
      "Epoch 21/100\n",
      "6/6 - 1s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.2949 - val_accuracy: 0.9688 - 1s/epoch - 241ms/step\n",
      "Epoch 22/100\n",
      "6/6 - 1s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.2989 - val_accuracy: 0.9688 - 746ms/epoch - 124ms/step\n",
      "Epoch 23/100\n",
      "6/6 - 0s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2981 - val_accuracy: 0.9688 - 169ms/epoch - 28ms/step\n",
      "Epoch 24/100\n",
      "6/6 - 0s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2978 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 25/100\n",
      "6/6 - 0s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9688 - 148ms/epoch - 25ms/step\n",
      "Epoch 26/100\n",
      "6/6 - 0s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2900 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 27/100\n",
      "6/6 - 0s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2827 - val_accuracy: 0.9688 - 150ms/epoch - 25ms/step\n",
      "Epoch 28/100\n",
      "6/6 - 0s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2900 - val_accuracy: 0.9688 - 164ms/epoch - 27ms/step\n",
      "Epoch 29/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2984 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 30/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.9688 - 171ms/epoch - 28ms/step\n",
      "Epoch 31/100\n",
      "6/6 - 0s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9688 - 133ms/epoch - 22ms/step\n",
      "Epoch 32/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2859 - val_accuracy: 0.9688 - 172ms/epoch - 29ms/step\n",
      "Epoch 33/100\n",
      "6/6 - 0s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9688 - 177ms/epoch - 29ms/step\n",
      "Epoch 34/100\n",
      "6/6 - 0s - loss: 0.0094 - accuracy: 0.9961 - val_loss: 0.2802 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 35/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2818 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 36/100\n",
      "6/6 - 0s - loss: 0.0084 - accuracy: 0.9961 - val_loss: 0.2865 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 37/100\n",
      "6/6 - 0s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2812 - val_accuracy: 0.9531 - 153ms/epoch - 25ms/step\n",
      "Epoch 38/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2841 - val_accuracy: 0.9531 - 132ms/epoch - 22ms/step\n",
      "Epoch 39/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.9531 - 143ms/epoch - 24ms/step\n",
      "Epoch 40/100\n",
      "6/6 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2933 - val_accuracy: 0.9531 - 163ms/epoch - 27ms/step\n",
      "Epoch 41/100\n",
      "6/6 - 0s - loss: 0.0095 - accuracy: 0.9961 - val_loss: 0.2873 - val_accuracy: 0.9531 - 141ms/epoch - 23ms/step\n",
      "Epoch 42/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9531 - 135ms/epoch - 23ms/step\n",
      "Epoch 43/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2592 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 44/100\n",
      "6/6 - 0s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2620 - val_accuracy: 0.9531 - 169ms/epoch - 28ms/step\n",
      "Epoch 45/100\n",
      "6/6 - 0s - loss: 0.0055 - accuracy: 0.9961 - val_loss: 0.2634 - val_accuracy: 0.9531 - 159ms/epoch - 26ms/step\n",
      "Epoch 46/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 0.9531 - 186ms/epoch - 31ms/step\n",
      "Epoch 47/100\n",
      "6/6 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9531 - 211ms/epoch - 35ms/step\n",
      "Epoch 48/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.9688 - 146ms/epoch - 24ms/step\n",
      "Epoch 49/100\n",
      "6/6 - 0s - loss: 0.0134 - accuracy: 0.9922 - val_loss: 0.2157 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 50/100\n",
      "6/6 - 0s - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.2227 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 51/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2374 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 52/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2387 - val_accuracy: 0.9531 - 116ms/epoch - 19ms/step\n",
      "Epoch 53/100\n",
      "6/6 - 0s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2327 - val_accuracy: 0.9531 - 112ms/epoch - 19ms/step\n",
      "Epoch 54/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 55/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2152 - val_accuracy: 0.9688 - 123ms/epoch - 20ms/step\n",
      "Epoch 56/100\n",
      "6/6 - 0s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2332 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 57/100\n",
      "6/6 - 0s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9531 - 122ms/epoch - 20ms/step\n",
      "Epoch 58/100\n",
      "6/6 - 0s - loss: 0.0061 - accuracy: 0.9961 - val_loss: 0.2534 - val_accuracy: 0.9531 - 119ms/epoch - 20ms/step\n",
      "Epoch 59/100\n",
      "6/6 - 0s - loss: 0.0078 - accuracy: 0.9961 - val_loss: 0.2568 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 60/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2498 - val_accuracy: 0.9531 - 146ms/epoch - 24ms/step\n",
      "Epoch 61/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2413 - val_accuracy: 0.9531 - 125ms/epoch - 21ms/step\n",
      "Epoch 62/100\n",
      "6/6 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9688 - 162ms/epoch - 27ms/step\n",
      "Epoch 63/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2339 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 64/100\n",
      "6/6 - 0s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 65/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2124 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 66/100\n",
      "6/6 - 0s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2130 - val_accuracy: 0.9688 - 123ms/epoch - 20ms/step\n",
      "Epoch 67/100\n",
      "6/6 - 0s - loss: 0.0066 - accuracy: 0.9961 - val_loss: 0.2089 - val_accuracy: 0.9688 - 117ms/epoch - 19ms/step\n",
      "Epoch 68/100\n",
      "6/6 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 69/100\n",
      "6/6 - 0s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.2014 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 70/100\n",
      "6/6 - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.2131 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 71/100\n",
      "6/6 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.2276 - val_accuracy: 0.9531 - 151ms/epoch - 25ms/step\n",
      "Epoch 72/100\n",
      "6/6 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2364 - val_accuracy: 0.9531 - 155ms/epoch - 26ms/step\n",
      "Epoch 73/100\n",
      "6/6 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.2417 - val_accuracy: 0.9531 - 139ms/epoch - 23ms/step\n",
      "Epoch 74/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 0.9531 - 137ms/epoch - 23ms/step\n",
      "Epoch 75/100\n",
      "6/6 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9531 - 165ms/epoch - 27ms/step\n",
      "Epoch 76/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 0.9531 - 134ms/epoch - 22ms/step\n",
      "Epoch 77/100\n",
      "6/6 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9531 - 126ms/epoch - 21ms/step\n",
      "Epoch 78/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2818 - val_accuracy: 0.9531 - 133ms/epoch - 22ms/step\n",
      "Epoch 79/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2759 - val_accuracy: 0.9531 - 129ms/epoch - 22ms/step\n",
      "Epoch 80/100\n",
      "6/6 - 0s - loss: 8.5299e-04 - accuracy: 1.0000 - val_loss: 0.2568 - val_accuracy: 0.9688 - 131ms/epoch - 22ms/step\n",
      "Epoch 81/100\n",
      "6/6 - 0s - loss: 0.0075 - accuracy: 0.9961 - val_loss: 0.2349 - val_accuracy: 0.9688 - 117ms/epoch - 19ms/step\n",
      "Epoch 82/100\n",
      "6/6 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 0.9688 - 135ms/epoch - 23ms/step\n",
      "Epoch 83/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2297 - val_accuracy: 0.9688 - 122ms/epoch - 20ms/step\n",
      "Epoch 84/100\n",
      "6/6 - 0s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.2282 - val_accuracy: 0.9688 - 112ms/epoch - 19ms/step\n",
      "Epoch 85/100\n",
      "6/6 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2282 - val_accuracy: 0.9688 - 111ms/epoch - 19ms/step\n",
      "Epoch 86/100\n",
      "6/6 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2280 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 87/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2271 - val_accuracy: 0.9688 - 116ms/epoch - 19ms/step\n",
      "Epoch 88/100\n",
      "6/6 - 0s - loss: 8.9481e-04 - accuracy: 1.0000 - val_loss: 0.2205 - val_accuracy: 0.9688 - 125ms/epoch - 21ms/step\n",
      "Epoch 89/100\n",
      "6/6 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2168 - val_accuracy: 0.9688 - 119ms/epoch - 20ms/step\n",
      "Epoch 90/100\n",
      "6/6 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9688 - 117ms/epoch - 20ms/step\n",
      "Epoch 91/100\n",
      "6/6 - 0s - loss: 9.3541e-04 - accuracy: 1.0000 - val_loss: 0.2170 - val_accuracy: 0.9688 - 109ms/epoch - 18ms/step\n",
      "Epoch 92/100\n",
      "6/6 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2175 - val_accuracy: 0.9688 - 139ms/epoch - 23ms/step\n",
      "Epoch 93/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2185 - val_accuracy: 0.9688 - 146ms/epoch - 24ms/step\n",
      "Epoch 94/100\n",
      "6/6 - 0s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9688 - 126ms/epoch - 21ms/step\n",
      "Epoch 95/100\n",
      "6/6 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9688 - 128ms/epoch - 21ms/step\n",
      "Epoch 96/100\n",
      "6/6 - 0s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.2995 - val_accuracy: 0.9688 - 113ms/epoch - 19ms/step\n",
      "Epoch 97/100\n",
      "6/6 - 0s - loss: 0.0057 - accuracy: 0.9961 - val_loss: 0.3148 - val_accuracy: 0.9531 - 123ms/epoch - 21ms/step\n",
      "Epoch 98/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2915 - val_accuracy: 0.9531 - 146ms/epoch - 24ms/step\n",
      "Epoch 99/100\n",
      "6/6 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2812 - val_accuracy: 0.9531 - 115ms/epoch - 19ms/step\n",
      "Epoch 100/100\n",
      "6/6 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9531 - 129ms/epoch - 22ms/step\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1368 - accuracy: 0.9750\n",
      "input_12 Quantized method: max-min  Values max: 23339.0 min: -23248.0 dec bit -8\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "conv1d_44 Quantized method: max-min  Values max: 13285.554 min: -13110.609 dec bit -7\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "batch_normalization_44 Quantized method: max-min  Values max: 7.4835825 min: -8.181089 dec bit 3\n",
      "dropout_66 Quantized method: max-min  Values max: 7.4835825 min: -8.181089 dec bit 3\n",
      "re_lu_55 Quantized method: max-min  Values max: 7.4835825 min: -8.181089 dec bit 3\n",
      "max_pooling1d_44 Quantized method: max-min  Values max: 7.4835825 min: -8.181089 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_45 Quantized method: max-min  Values max: 6.652037 min: -8.322833 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_46 Quantized method: max-min  Values max: 7.4578342 min: -4.697908 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_45 Quantized method: max-min  Values max: 6.223192 min: -4.7233386 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_46 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "dropout_67 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "dropout_68 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "re_lu_56 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "re_lu_57 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "max_pooling1d_47 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "max_pooling1d_45 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "max_pooling1d_46 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "dropout_69 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "concatenate_11 Quantized method: max-min  Values max: 7.590832 min: -5.252475 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "conv1d_47 Quantized method: max-min  Values max: 13.485227 min: -8.284726 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "batch_normalization_47 Quantized method: max-min  Values max: 5.1558824 min: -4.5095778 dec bit 4\n",
      "re_lu_58 Quantized method: max-min  Values max: 5.1558824 min: -4.5095778 dec bit 4\n",
      "dropout_70 Quantized method: max-min  Values max: 5.1558824 min: -4.5095778 dec bit 4\n",
      "flatten_11 Quantized method: max-min  Values max: 5.1558824 min: -4.5095778 dec bit 4\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_22 Quantized method: max-min  Values max: 8.2664385 min: -6.684762 dec bit 3\n",
      "dropout_71 Quantized method: max-min  Values max: 8.2664385 min: -6.684762 dec bit 3\n",
      "re_lu_59 Quantized method: max-min  Values max: 8.2664385 min: -6.684762 dec bit 3\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "dense_23 Quantized method: max-min  Values max: 14.314482 min: -11.699246 dec bit 3\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "softmax_11 Quantized method: max-min  Values max: 0.99999976 min: 1.9301262e-11 dec bit 7\n",
      "set dec bit 4 for the input of concatenate_11 : ['max_pooling1d_45', 'max_pooling1d_46', 'dropout_69']\n",
      "quantisation list {'input_12': [-8, 0], 'conv1d_44': [4, 0], 'batch_normalization_44': [4, 0], 'dropout_66': [4, 0], 're_lu_55': [4, 0], 'max_pooling1d_44': [4, 0], 'conv1d_45': [4, 0], 'conv1d_46': [4, 0], 'batch_normalization_45': [4, 0], 'batch_normalization_46': [4, 0], 'dropout_67': [4, 0], 'dropout_68': [4, 0], 're_lu_56': [4, 0], 're_lu_57': [4, 0], 'max_pooling1d_47': [4, 0], 'max_pooling1d_45': [4, 0], 'max_pooling1d_46': [4, 0], 'dropout_69': [4, 0], 'concatenate_11': [4, 0], 'conv1d_47': [4, 0], 'batch_normalization_47': [4, 0], 're_lu_58': [4, 0], 'dropout_70': [4, 0], 'flatten_11': [4, 0], 'dense_22': [3, 0], 'dropout_71': [3, 0], 're_lu_59': [3, 0], 'dense_23': [3, 0], 'softmax_11': [7, 0]}\n",
      "fusing batch normalization to conv1d_44\n",
      "original weight max 0.16669722 min -0.14771874\n",
      "original bias max 1.1255009e-07 min -2.3469038e-07\n",
      "fused weight max 0.00025263522 min -0.00023507411\n",
      "fused bias max 0.0811914 min -0.070109464\n",
      "quantizing weights for layer conv1d_44\n",
      "    tensor_conv1d_44_kernel_0 dec bit 18\n",
      "    tensor_conv1d_44_bias_0 dec bit 10\n",
      "quantizing weights for layer batch_normalization_44\n",
      "fusing batch normalization to conv1d_45\n",
      "original weight max 0.16075134 min -0.16105288\n",
      "original bias max 9.927729e-05 min -0.00020815527\n",
      "fused weight max 0.2724246 min -0.29120144\n",
      "fused bias max 0.9736572 min -0.8140568\n",
      "quantizing weights for layer conv1d_45\n",
      "    tensor_conv1d_45_kernel_0 dec bit 8\n",
      "    tensor_conv1d_45_bias_0 dec bit 7\n",
      "fusing batch normalization to conv1d_46\n",
      "original weight max 0.19906588 min -0.19980519\n",
      "original bias max 0.000119171695 min -0.00012713141\n",
      "fused weight max 0.31304282 min -0.32663885\n",
      "fused bias max 1.1172905 min -1.0844543\n",
      "quantizing weights for layer conv1d_46\n",
      "    tensor_conv1d_46_kernel_0 dec bit 8\n",
      "    tensor_conv1d_46_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_45\n",
      "quantizing weights for layer batch_normalization_46\n",
      "fusing batch normalization to conv1d_47\n",
      "original weight max 0.15619482 min -0.15437566\n",
      "original bias max 0.00015289219 min -0.00013629426\n",
      "fused weight max 0.13552354 min -0.1438472\n",
      "fused bias max 1.3275859 min -0.93754685\n",
      "quantizing weights for layer conv1d_47\n",
      "    tensor_conv1d_47_kernel_0 dec bit 9\n",
      "    tensor_conv1d_47_bias_0 dec bit 6\n",
      "quantizing weights for layer batch_normalization_47\n",
      "quantizing weights for layer dense_22\n",
      "    tensor_dense_22_kernel_0 dec bit 9\n",
      "    tensor_dense_22_bias_0 dec bit 12\n",
      "quantizing weights for layer dense_23\n",
      "    tensor_dense_23_kernel_0 dec bit 8\n",
      "    tensor_dense_23_bias_0 dec bit 12\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, loss, accuracy, X_test\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m--> 122\u001b[0m     model, loss, accuracy, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.85\u001b[39m:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m model \u001b[38;5;241m=\u001b[39m inception_model()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# model = rnn_model()\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# 训练模型  \u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m    117\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:904\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    909\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    910\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/nnom_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def inception_model():\n",
    "    # 定义输入层\n",
    "    inputs = Input(shape=(200, 3))\n",
    "\n",
    "    x = Conv1D(32, kernel_size=(9), strides=(2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPool1D(2, strides=2)(x)\n",
    "\n",
    "    # inception - 1\n",
    "    x1 = Conv1D(32, kernel_size=(5), strides=(1), padding=\"same\")(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    x1 = ReLU()(x1)\n",
    "    x1 = MaxPool1D(2, strides=2)(x1)\n",
    "\n",
    "    # inception - 2\n",
    "    x2 = Conv1D(32, kernel_size=(3), strides=(1), padding=\"same\")(x)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    x2 = ReLU()(x2)\n",
    "    x2 = MaxPool1D(2, strides=2)(x2)\n",
    "\n",
    "    # inception - 3\n",
    "    x3 = MaxPool1D(2, strides=2)(x)\n",
    "    x3 = Dropout(0.2)(x3)\n",
    "\n",
    "    # concate all inception layers\n",
    "    x = concatenate([ x1, x2,x3], axis=-1)\n",
    "\n",
    "    #conclusion\n",
    "    x = Conv1D(48, kernel_size=(3), strides=(1), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    #x = MaxPool1D(2, strides=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # our netowrk is not that deep, so a hidden fully connected layer is introduce\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dense(class_num)(x)\n",
    "    outputs = layers.Softmax()(x)\n",
    "\n",
    "    # 创建模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # 编译模型，优化器使用adam，损失函数使用交叉熵损失函数，评估标准为准确率  \n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def rnn_model():\n",
    "    inputs = Input(shape=(200, 3))\n",
    "    x = Conv1D(9, kernel_size=(9), strides=(3), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # you can use either of the format below.\n",
    "    # x = RNN(SimpleRNNCell(16), return_sequences=True)(x)\n",
    "    # x = SimpleRNN(16, return_sequences=True)(x)\n",
    "\n",
    "    # x2 = RNN(LSTMCell(32), return_sequences=True)(x)\n",
    "    # x1 = LSTM(32, return_sequences=True, go_backwards=True)(x)\n",
    "    # x = concatenate([x1, x2], axis=-1)\n",
    "    #\n",
    "    # # Bidirectional with concatenate. (not working yet)\n",
    "    # x1 = RNN(GRUCell(16), return_sequences=True)(x)\n",
    "    # x2 = GRU(16, return_sequences=True, go_backwards=True)(x)\n",
    "    # x = concatenate([x1, x2], axis=-1)\n",
    "\n",
    "\n",
    "    # Bidirectional with concatenate. (not working yet)\n",
    "\n",
    "    # x1 = LSTM(32, return_sequences=True)(x)\n",
    "    # x2 = LSTM(32, return_sequences=True, go_backwards=True)(x)\n",
    "    # x = add([x1, x2])\n",
    "    x = LSTM(32, return_sequences=True)(x)\n",
    "    x1 = GRU(32, return_sequences=True)(x)\n",
    "    x2 = GRU(32, return_sequences=True, go_backwards=True)(x)\n",
    "    x = concatenate([x1, x2], axis=-1)\n",
    "\n",
    "    x = GRU(32, return_sequences=True)(x)\n",
    "    x = GRU(32, return_sequences=True)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(class_num)(x)\n",
    "    predictions = Softmax()(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train():\n",
    "    # 1. 分割为训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(total_data, total_labels, test_size=0.2, shuffle=True, stratify=total_labels)\n",
    "\n",
    "    # 2. 进一步分割训练集为训练集和验证集\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, stratify=y_train)\n",
    "\n",
    "\n",
    "    model = inception_model()\n",
    "    # model = rnn_model()\n",
    "    \n",
    "    # 训练模型  \n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=50, validation_data=(X_val, y_val), verbose=2)\n",
    "\n",
    "    # 评估模型\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return model, loss, accuracy, X_test\n",
    "\n",
    "for i in range(20):\n",
    "    model, loss, accuracy, X_test = train()\n",
    "    if accuracy > 0.85:\n",
    "        from keras.models import load_model\n",
    "        # 保存模型\n",
    "        from keras.models import save_model\n",
    "        save_model_name = f\"./save_model/model_{i}_loss_{loss}_acc_{accuracy}\"\n",
    "        save_model(model, save_model_name + \".keras\")\n",
    "        \n",
    "        L_model = load_model(save_model_name + \".keras\")\n",
    "\n",
    "        # 假设generate_model函数已经定义在nnom模块中\n",
    "        generate_model(L_model, X_test, name = save_model_name + \".h\")\n",
    "    del model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14c97e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "from keras.models import save_model\n",
    "save_model_name = \"best_model\"\n",
    "save_model(model, save_model_name + \".keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49381ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "L_model = load_model(save_model_name + \".keras\")\n",
    "\n",
    "loss, accuracy = L_model.evaluate(X_test, y_test)\n",
    "\n",
    "# 假设generate_model函数已经定义在nnom模块中\n",
    "generate_model(L_model, X_test, name = save_model_name + \".h\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
